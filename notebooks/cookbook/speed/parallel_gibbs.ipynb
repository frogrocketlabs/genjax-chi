{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Parallel Gibbs Sampling\n",
    "subtitle: Surprising ways in which parallelism can be introduced\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import genjax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from genjax import ChoiceMapBuilder as C\n",
    "from jax import jit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have mostly shown how to use GenJAX to run simulations in parallel. Whether it was a generative function on several random keys, on different arguments, they all had a similar flavor of \"simply duplicating particles for inference.\"\n",
    "\n",
    "Here we will show a different kind of example where parallelism can be used for better inference which has a very different flavor: we will do a type of MCMC update to a trace where the move itself benefits from parallel acceleration thanks to the structure of the generative function to which the update is performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first create a simple HMM and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_chain = 50\n",
    "state_size = 100\n",
    "number_runs = 1000\n",
    "# for numerical stability of the HMM, ensuring that the eigenvalues of the transition matrices are around 1.\n",
    "magic_number = jnp.exp(1)\n",
    "normalizer = 1.0 / jnp.sqrt(state_size / magic_number)\n",
    "transition_matrix = (\n",
    "    jax.random.normal(jax.random.PRNGKey(0), (state_size, state_size)) * normalizer\n",
    ")\n",
    "observation_matrix = (\n",
    "    jax.random.normal(jax.random.PRNGKey(42), (state_size, state_size)) * normalizer\n",
    ")\n",
    "latent_variance = jnp.eye(state_size)\n",
    "obs_variance = jnp.eye(state_size)\n",
    "initial_state = jax.random.normal(jax.random.PRNGKey(0), (state_size,))\n",
    "\n",
    "\n",
    "@genjax.gen\n",
    "def hmm_step(x, _):\n",
    "    new_x = (\n",
    "        genjax.mv_normal(jnp.matmul(transition_matrix, x), latent_variance) @ \"new_x\"\n",
    "    )\n",
    "    _ = genjax.mv_normal(jnp.matmul(observation_matrix, new_x), obs_variance) @ \"obs\"\n",
    "    return new_x, None\n",
    "\n",
    "\n",
    "hmm = hmm_step.scan(n=length_chain)\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "jitted = jit(hmm.repeat(n=number_runs).simulate)\n",
    "trace = jitted(key, (initial_state, None))\n",
    "trace.get_choices()\n",
    "%timeit jitted(key, (initial_state, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add observervations and run the default importance sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chm = jax.vmap(\n",
    "    lambda idx: C[idx, \"obs\"].set(\n",
    "        idx.astype(float) * jnp.arange(state_size) / state_size\n",
    "    )\n",
    ")(jnp.arange(length_chain))\n",
    "\n",
    "\n",
    "jitted = jit(lambda key: hmm.importance(key, chm, (initial_state, None)))\n",
    "jitted(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_parallel_update_logic():\n",
    "    simpled_jitted = jit(lambda key: hmm.simulate(key, (initial_state, None)))\n",
    "    simple_tr = simpled_jitted(key)\n",
    "    vars_to_update = simple_tr.get_choices()[..., \"new_x\"]\n",
    "\n",
    "    magic_c_matrix = jnp.matmul(transition_matrix.T, jnp.linalg.inv(latent_variance))\n",
    "\n",
    "    # single update\n",
    "    vars_to_update = vars_to_update.at[2].set(\n",
    "        jnp.matmul(magic_c_matrix, vars_to_update[2])\n",
    "    )\n",
    "\n",
    "    # multiple updates\n",
    "    parity = 0\n",
    "    idx_to_update = jnp.arange(length_chain)[jnp.arange(length_chain) % 2 == parity]\n",
    "    # maybe something like that\n",
    "    updating_vals = (magic_c_matrix.T @ vars_to_update[idx_to_update].T).T\n",
    "    vars_to_update = vars_to_update.at[idx_to_update].set(updating_vals)\n",
    "\n",
    "    # simple test\n",
    "    vars_to_update = jnp.zeros((length_chain, state_size))\n",
    "    updating_vals = (magic_c_matrix.T @ (vars_to_update[idx_to_update] + 1).T).T\n",
    "    print(updating_vals.shape)\n",
    "    vars_to_update = vars_to_update.at[idx_to_update].set(updating_vals)\n",
    "    return vars_to_update\n",
    "\n",
    "\n",
    "test_parallel_update_logic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs_update(args, key):\n",
    "    trace, parity = args\n",
    "    vars_to_update = trace.get_choices()[..., \"new_x\"]\n",
    "    idx_to_update = jnp.arange(length_chain)[jnp.arange(length_chain) % 2 == parity]\n",
    "\n",
    "    # TODO: actual gibbs update, and need to use conditional rule from GaussPPL\n",
    "    magic_c_matrix = jnp.matmul(transition_matrix.T, jnp.linalg.inv(latent_variance))\n",
    "    updating_vals = (magic_c_matrix.T @ vars_to_update[idx_to_update].T).T\n",
    "    vars_to_update = vars_to_update.at[idx_to_update].set(updating_vals)\n",
    "\n",
    "    # TODO: need to actually return a proper trace in order to be able to iterate the gibbs update, and need to return the vars_to_update\n",
    "    return (\n",
    "        genjax._src.generative_functions.static.StaticTrace(\n",
    "            trace.get_gen_fn(),\n",
    "            trace.get_args(),\n",
    "            trace.get_retval(),\n",
    "            genjax.Pytree.field(\n",
    "                default_factory=genjax._src.generative_functions.static.AddressVisitor\n",
    "            ),\n",
    "            genjax.Pytree.field(default_factory=list),\n",
    "            trace.get_score(),\n",
    "        ),\n",
    "        (parity + 1) % 2,\n",
    "    )\n",
    "\n",
    "\n",
    "# gibbs_update((simple_tr, 0), key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now test inference using the parallel Gibbs update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use vmap to update all chains in parallel, and use scan to keep updating until convergence\n",
    "\n",
    "number_gibbs_sweep = 1000\n",
    "keys = jax.random.split(key, number_gibbs_sweep)\n",
    "# args = (simple_tr, 0)\n",
    "# TODO: need to fix a ShapedArray(bool[50]) and the returned trace\n",
    "# iterated_gibbs = lambda keys: jax.lax.scan(gibbs_update, args, keys)\n",
    "# jitted_gibbs = jax.jit(iterated_gibbs)\n",
    "\n",
    "# TODO: now need a parallel version for all the different initial traces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the quality of the inference in this case as it's a rare instance when one can do exact inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: exact inference using GaussPPL and comparison with Gibbs sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the time difference between the exact and approximate method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
