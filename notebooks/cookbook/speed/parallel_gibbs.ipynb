{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Parallel Gibbs Sampling\n",
    "subtitle: Surprising ways in which parallelism can be introduced\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import genjax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from genjax import ChoiceMapBuilder as C\n",
    "from genjax._src.generative_functions.combinators.scan import ScanTrace\n",
    "from genjax._src.generative_functions.static import StaticTrace\n",
    "from jax import jit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have mostly shown how to use GenJAX to run simulations in parallel. Whether it was a generative function on several random keys, on different arguments, they all had a similar flavor of \"simply duplicating particles for inference.\"\n",
    "\n",
    "Here we will show a different kind of example where parallelism can be used for better inference which has a very different flavor: we will do a type of MCMC update to a trace where the move itself benefits from parallel acceleration thanks to the structure of the generative function to which the update is performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first create a simple HMM and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_chain = 50\n",
    "state_size = 100\n",
    "number_runs = 1000\n",
    "# for numerical stability of the HMM, ensuring that the eigenvalues of the transition matrices are around 1.\n",
    "magic_number = jnp.exp(1)\n",
    "normalizer = 1.0 / jnp.sqrt(state_size / magic_number)\n",
    "transition_matrix = (\n",
    "    jax.random.normal(jax.random.PRNGKey(0), (state_size, state_size)) * normalizer\n",
    ")\n",
    "observation_matrix = (\n",
    "    jax.random.normal(jax.random.PRNGKey(42), (state_size, state_size)) * normalizer\n",
    ")\n",
    "latent_variance = jnp.eye(state_size)\n",
    "obs_variance = jnp.eye(state_size)\n",
    "initial_state = jax.random.normal(jax.random.PRNGKey(0), (state_size,))\n",
    "\n",
    "\n",
    "@genjax.gen\n",
    "def hmm_step(x, _):\n",
    "    new_x = (\n",
    "        genjax.mv_normal(jnp.matmul(transition_matrix, x), latent_variance) @ \"new_x\"\n",
    "    )\n",
    "    _ = genjax.mv_normal(jnp.matmul(observation_matrix, new_x), obs_variance) @ \"obs\"\n",
    "    return new_x, None\n",
    "\n",
    "\n",
    "hmm = hmm_step.scan(n=length_chain)\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "jitted = jit(hmm.repeat(n=number_runs).simulate)\n",
    "trace = jitted(key, (initial_state, None))\n",
    "trace.get_choices()\n",
    "%timeit jitted(key, (initial_state, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add observervations and run the default importance sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chm = jax.vmap(\n",
    "    lambda idx: C[idx, \"obs\"].set(\n",
    "        idx.astype(float) * jnp.arange(state_size) / state_size\n",
    "    )\n",
    ")(jnp.arange(length_chain))\n",
    "\n",
    "\n",
    "jitted = jit(lambda key: hmm.importance(key, chm, (initial_state, None)))\n",
    "jitted(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_parallel_update_logic():\n",
    "    simpled_jitted = jit(lambda key: hmm.simulate(key, (initial_state, None)))\n",
    "    simple_tr = simpled_jitted(key)\n",
    "    vars_to_update = simple_tr.get_choices()[..., \"new_x\"]\n",
    "\n",
    "    magic_c_matrix = jnp.matmul(transition_matrix.T, jnp.linalg.inv(latent_variance))\n",
    "\n",
    "    # single update\n",
    "    vars_to_update = vars_to_update.at[2].set(\n",
    "        jnp.matmul(magic_c_matrix, vars_to_update[2])\n",
    "    )\n",
    "\n",
    "    # multiple updates\n",
    "    parity = 0\n",
    "    idx_to_update = jnp.arange(length_chain)[jnp.arange(length_chain) % 2 == parity]\n",
    "    idx_to_update = jnp.arange(start=parity, step=2, end=length_chain)\n",
    "    # TODO: maybe something like that. also maybe faster/simpler with vmap?\n",
    "    updating_vals = (magic_c_matrix.T @ vars_to_update[idx_to_update].T).T\n",
    "    vars_to_update = vars_to_update.at[idx_to_update].set(updating_vals)\n",
    "\n",
    "    # simple test\n",
    "    vars_to_update = jnp.zeros((length_chain, state_size))\n",
    "    updating_vals = (magic_c_matrix.T @ (vars_to_update[idx_to_update] + 1).T).T\n",
    "    print(updating_vals.shape)\n",
    "    vars_to_update = vars_to_update.at[idx_to_update].set(updating_vals)\n",
    "    return vars_to_update\n",
    "\n",
    "\n",
    "test_parallel_update_logic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs_update(args, key):\n",
    "    trace, parity = args\n",
    "    vars_to_update = trace.get_choices()[..., \"new_x\"]\n",
    "    # idx_to_update = jnp.arange(length_chain)[jnp.arange(length_chain) % 2 == parity]\n",
    "    idx_to_update = jnp.arange(start=1, step=2, stop=length_chain)\n",
    "\n",
    "    # TODO: actual gibbs update, and need to use conditional rule from GaussPPL\n",
    "    magic_c_matrix = jnp.matmul(transition_matrix.T, jnp.linalg.inv(latent_variance))\n",
    "    updating_vals = (magic_c_matrix.T @ vars_to_update[idx_to_update].T).T\n",
    "    vars_to_update = vars_to_update.at[idx_to_update].set(updating_vals)\n",
    "\n",
    "    # TODO: need to actually return a proper trace in order to be able to iterate the gibbs update, and need to return the vars_to_update\n",
    "    updated_one = trace.inner.subtraces[1]  # TODO:\n",
    "    inner = StaticTrace(\n",
    "        trace.inner.gen_fn,\n",
    "        trace.inner.args,\n",
    "        trace.inner.retval,\n",
    "        trace.inner.addresses,\n",
    "        [trace.inner.subtraces[0], updated_one],\n",
    "        trace.inner.score,\n",
    "    )\n",
    "    return (\n",
    "        ScanTrace(trace.get_gen_fn(), inner, trace.args, trace.retval, trace.score),\n",
    "        (parity + 1) % 2,\n",
    "    ), None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now test inference using the parallel Gibbs update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_tr = hmm.simulate(key, (initial_state, None))\n",
    "\n",
    "number_gibbs_sweep = 1000\n",
    "keys = jax.random.split(key, number_gibbs_sweep)\n",
    "args = (simple_tr, 0)\n",
    "jitted_gibbs = jax.jit(lambda keys: jax.lax.scan(gibbs_update, args, keys))\n",
    "jitted_gibbs(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `vmap` to launch different MCMC chains in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = jax.random.split(key, number_runs)\n",
    "initial_states = jax.vmap(lambda key: jax.random.normal(key, (state_size,)))(keys)\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "keys = jax.random.split(key, number_runs)\n",
    "initial_traces = jax.jit(\n",
    "    jax.vmap(lambda initial_state, key: hmm.simulate(key, (initial_state, None)))\n",
    ")(initial_states, keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = jax.random.split(key)\n",
    "keys = jax.random.split(key, number_gibbs_sweep)\n",
    "jitted_gibbs = jax.jit(lambda tr, keys: jax.lax.scan(gibbs_update, (tr, 0), keys))\n",
    "# TODO: need to feed the traces and keys with the right format\n",
    "# traces = jitted_gibbs(initial_traces, keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the quality of the inference in this case as it's a rare instance when one can do exact inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: exact inference using GaussPPL and comparison with Gibbs sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the time difference between the exact and approximate method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
