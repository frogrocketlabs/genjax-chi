{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Differentiating probabilistic programs\n",
    "subtitle: How to take drastic differentiating measures by differentiating measures\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and constants\n",
    "import genstudio.plot as Plot\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from genjax._src.adev.core import Dual, expectation\n",
    "from genjax._src.adev.primitives import flip_enum, normal_reparam\n",
    "\n",
    "key = jax.random.key(314159)\n",
    "EPOCHS = 400\n",
    "sigma = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "def noisy_jax_model(key, theta):\n",
    "    b = jax.random.bernoulli(key, theta)\n",
    "    return jax.lax.cond(\n",
    "        b,\n",
    "        lambda theta: jax.random.normal(key) * sigma * theta,\n",
    "        lambda theta: jax.random.normal(key) * sigma + theta / 2,\n",
    "        theta,\n",
    "    )\n",
    "\n",
    "\n",
    "def expected_val(theta):\n",
    "    return (theta - theta**2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples\n",
    "thetas = jnp.arange(0.0, 1.0, 0.0005)\n",
    "keys = jax.random.split(key, len(thetas))\n",
    "\n",
    "noisy_samples = jax.vmap(noisy_jax_model, in_axes=(0, 0))(keys, thetas)\n",
    "\n",
    "plot_options = Plot.new(\n",
    "    Plot.color_legend(),\n",
    "    {\"x\": {\"label\": \"θ\"}, \"y\": {\"label\": \"y\"}},\n",
    "    Plot.aspect_ratio(1),\n",
    "    Plot.grid(),\n",
    ")\n",
    "\n",
    "(\n",
    "    Plot.dot({\"x\": thetas, \"y\": noisy_samples}, fill=Plot.constantly(\"samples\"), r=2)\n",
    "    + plot_options\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding exact expectation\n",
    "exact_vals = jax.vmap(expected_val)(thetas)\n",
    "\n",
    "expected_value_plot = (\n",
    "    Plot.line(\n",
    "        {\"x\": thetas, \"y\": exact_vals},\n",
    "        strokeWidth=2,\n",
    "        stroke=Plot.constantly(\"Expected value\"),\n",
    "    )\n",
    "    + plot_options\n",
    ")\n",
    "\n",
    "(\n",
    "    Plot.dot({\"x\": thetas, \"y\": noisy_samples}, fill=Plot.constantly(\"samples\"), r=2)\n",
    "    + expected_value_plot\n",
    "    + plot_options\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JAX computed exact gradients\n",
    "grad_exact = jax.jit(jax.grad(expected_val))\n",
    "theta_tangent_points = [0.1, 0.3, 0.45]\n",
    "\n",
    "plot_thetas = jnp.linspace(0, 1, 400)\n",
    "y = expected_val(plot_thetas)\n",
    "\n",
    "# Optimization on ideal curve\n",
    "arg = 0.2\n",
    "vals = []\n",
    "arg_list = []\n",
    "for _ in range(EPOCHS):\n",
    "    grad_val = grad_exact(arg)\n",
    "    arg_list.append(arg)\n",
    "    vals.append(expected_val(arg))\n",
    "    arg = arg + 0.01 * grad_val\n",
    "    if arg < 0:\n",
    "        arg = 0\n",
    "        break\n",
    "    elif arg > 1:\n",
    "        arg = 1\n",
    "\n",
    "color1 = \"#D4CC47\"\n",
    "color2 = \"#FB575D\"\n",
    "\n",
    "\n",
    "def hex_to_RGB(hex_str):\n",
    "    \"\"\"#FFFFFF -> [255,255,255]\"\"\"\n",
    "    return [int(hex_str[i : i + 2], 16) for i in range(1, 6, 2)]\n",
    "\n",
    "\n",
    "def get_color_gradient(c1, c2, n):\n",
    "    \"\"\"\n",
    "    Given two hex colors, returns a color gradient\n",
    "    with n colors.\n",
    "    \"\"\"\n",
    "    assert n > 1\n",
    "    c1_rgb = jnp.array(hex_to_RGB(c1)) / 255\n",
    "    c2_rgb = jnp.array(hex_to_RGB(c2)) / 255\n",
    "    mix_pcts = [x / (n - 1) for x in range(n)]\n",
    "    rgb_colors = [((1 - mix) * c1_rgb + (mix * c2_rgb)) for mix in mix_pcts]\n",
    "    return [\n",
    "        \"#\" + \"\".join([format(int(round(val * 255)), \"02x\") for val in item])\n",
    "        for item in rgb_colors\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    Plot.line({\"x\": list(range(EPOCHS)), \"y\": vals})\n",
    "    + {\"x\": {\"label\": \"Iteration\"}, \"y\": {\"label\": \"y\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tangent_line_plot(theta_tan):\n",
    "    slope = grad_exact(theta_tan)\n",
    "    y_intercept = expected_val(theta_tan) - slope * theta_tan\n",
    "    tangent_line = slope * plot_thetas + y_intercept\n",
    "    return Plot.line(\n",
    "        {\"x\": plot_thetas, \"y\": tangent_line},\n",
    "        opacity=0.75,\n",
    "        stroke=Plot.constantly(f\"Tangent at θ={theta_tan}\"),\n",
    "    )\n",
    "\n",
    "\n",
    "(\n",
    "    plot_options\n",
    "    + [tangent_line_plot(theta_tan) for theta_tan in theta_tangent_points]\n",
    "    + expected_value_plot\n",
    "    + Plot.domain([0, 1], [0, 0.4])\n",
    "    + Plot.title(\"Expectation curve and its Tangent Lines\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_tan = 0.3\n",
    "slope = grad_exact(theta_tan)\n",
    "slope_estimates = [slope + i / 20 for i in range(-4, 4)]\n",
    "y_intercept = expected_val(theta_tan) - slope * theta_tan\n",
    "tangent_line = slope * plot_thetas + y_intercept\n",
    "\n",
    "\n",
    "def slope_plot(slope_est):\n",
    "    y_intercept = expected_val(theta_tan) - slope_est * theta_tan\n",
    "    return Plot.line(\n",
    "        {\"x\": plot_thetas, \"y\": slope_est * plot_thetas + y_intercept},\n",
    "        strokeWidth=2,\n",
    "        stroke=Plot.constantly(\"Tangent estimate\"),\n",
    "    )\n",
    "\n",
    "\n",
    "exact_tangent_plot = Plot.line(\n",
    "    {\"x\": plot_thetas, \"y\": tangent_line},\n",
    "    strokeWidth=2,\n",
    "    stroke=Plot.constantly(\"Exact tangent at θ=0.3\"),\n",
    ")\n",
    "\n",
    "\n",
    "Plot.new(\n",
    "    # + Plot.domain([0, 1], [0, 0.4])\n",
    "    [slope_plot(slope_est) for slope_est in slope_estimates],\n",
    "    exact_tangent_plot,\n",
    "    Plot.dot({\"x\": thetas, \"y\": noisy_samples}, fill=Plot.constantly(\"Samples\"), r=2),\n",
    "    Plot.title(\"Expectation curve and Tangent Estimates at θ=0.3\"),\n",
    "    Plot.color_map({\n",
    "        \"Expected value\": \"blue\",\n",
    "        \"Tangent estimate\": \"rgba(255,165,0,0.3)\",\n",
    "        \"Exact tangent at θ=0.3\": \"rgba(255,165,0,1)\",\n",
    "        \"Samples\": \"teal\",\n",
    "    }),\n",
    "    expected_value_plot,\n",
    "    plot_options,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax_grad = jax.jit(jax.grad(noisy_jax_model, argnums=1))\n",
    "\n",
    "arg = 0.2\n",
    "vals = []\n",
    "grads = []\n",
    "for _ in range(EPOCHS):\n",
    "    key, subkey = jax.random.split(key)\n",
    "    grad_val = jax_grad(subkey, arg)\n",
    "    arg = arg + 0.01 * grad_val\n",
    "    vals.append(expected_val(arg))\n",
    "    grads.append(grad_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    Plot.line(\n",
    "        {\"x\": list(range(EPOCHS)), \"y\": vals},\n",
    "        stroke=Plot.constantly(\"Attempting gradient ascent with JAX\"),\n",
    "    )\n",
    "    + {\"x\": {\"label\": \"Iteration\"}, \"y\": {\"label\": \"y\"}}\n",
    "    + Plot.domainX([0, EPOCHS])\n",
    "    + Plot.title(\"Maximization of the expected value of a probabilistic function\")\n",
    "    + Plot.color_legend()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_tangent_points = [0.15, 0.3, 0.65, 0.8]\n",
    "\n",
    "plot_thetas = jnp.linspace(0, 1, 400)\n",
    "y = expected_val(plot_thetas)\n",
    "\n",
    "\n",
    "def theta_tangent_plot(theta_tan):\n",
    "    global key\n",
    "    key, subkey = jax.random.split(key)\n",
    "    slope = jax_grad(subkey, theta_tan)\n",
    "    y_intercept = expected_val(theta_tan) - slope * theta_tan\n",
    "    tangent_line = slope * plot_thetas + y_intercept\n",
    "    return Plot.line(\n",
    "        {\"x\": plot_thetas, \"y\": tangent_line},\n",
    "        opacity=0.75,\n",
    "        stroke=Plot.constantly(f\"Tangent estimate at θ={theta_tan}\"),\n",
    "    )\n",
    "\n",
    "\n",
    "(\n",
    "    plot_options\n",
    "    + [theta_tangent_plot(theta_tan) for theta_tan in theta_tangent_points]\n",
    "    + expected_value_plot\n",
    "    + Plot.domain([0, 1], [0, 0.4])\n",
    "    + Plot.title(\"Expectation curve and JAX-computed tangent estimates\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@expectation\n",
    "def flip_approx_loss(theta):\n",
    "    b = flip_enum(theta)\n",
    "    return jax.lax.cond(\n",
    "        b,\n",
    "        lambda theta: normal_reparam(0.0, sigma) * theta,\n",
    "        lambda theta: normal_reparam(theta / 2, sigma),\n",
    "        theta,\n",
    "    )\n",
    "\n",
    "\n",
    "adev_grad = jax.jit(flip_approx_loss.jvp_estimate)\n",
    "\n",
    "arg = 0.2\n",
    "adev_vals = []\n",
    "adev_grads = []\n",
    "for _ in range(EPOCHS):\n",
    "    key, subkey = jax.random.split(key)\n",
    "    grad_val = adev_grad(subkey, Dual(arg, 1.0)).tangent\n",
    "    adev_grads.append(grad_val)\n",
    "    arg = arg + 0.01 * grad_val\n",
    "    adev_vals.append(expected_val(arg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    Plot.line(\n",
    "        {\"x\": list(range(EPOCHS)), \"y\": vals},\n",
    "        stroke=Plot.constantly(\"Gradient ascent with JAX\"),\n",
    "    )\n",
    "    + Plot.line(\n",
    "        {\"x\": list(range(EPOCHS)), \"y\": adev_vals},\n",
    "        stroke=Plot.constantly(\"Gradient ascent with ADEV\"),\n",
    "    )\n",
    "    + {\"x\": {\"label\": \"Iteration\"}, \"y\": {\"label\": \"y\"}}\n",
    "    + Plot.domainX([0, EPOCHS])\n",
    "    + Plot.title(\"Maximization of the expected value of a probabilistic function\")\n",
    "    + Plot.color_legend()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    Plot.line(\n",
    "        {\"x\": list(range(EPOCHS)), \"y\": grads},\n",
    "        stroke=Plot.constantly(\"Gradients from JAX\"),\n",
    "    )\n",
    "    + Plot.line(\n",
    "        {\"x\": list(range(EPOCHS)), \"y\": adev_grads},\n",
    "        stroke=Plot.constantly(\"Gradients from ADEV\"),\n",
    "    )\n",
    "    + {\"x\": {\"label\": \"Iteration\"}, \"y\": {\"label\": \"y\"}}\n",
    "    + Plot.domainX([0, EPOCHS])\n",
    "    + Plot.title(\"Comparison of computed gradients JAX vs ADEV\")\n",
    "    + Plot.color_legend()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
