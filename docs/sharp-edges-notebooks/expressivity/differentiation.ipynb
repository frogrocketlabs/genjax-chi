{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference seems to be redoing a lot of work every time I run it again. Couldn't we make it faster by reusing what was done previously somehow?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to achieve this above is to add learning on top of inference. Often, this can be done in a variety of ways that fall under the umbrella of amortized inference. We will explore a few options here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's set up a basic deterministic model and recall how to do learning via gradient descent in that setting, in Jax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and extract the MNIST dataset\n",
    "!wget www.di.ens.fr/~lelarge/MNIST.tar.gz --no-check-certificate\n",
    "!tar -zxvf MNIST.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simple model and training loop for the MNIST dataset\n",
    "# Taken from the JAX quickstart guide\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random\n",
    "\n",
    "\n",
    "## Hyperparameters\n",
    "# A helper function to randomly initialize weights and biases\n",
    "# for a dense neural network layer\n",
    "def random_layer_params(m, n, key, scale=1e-2):\n",
    "    w_key, b_key = random.split(key)\n",
    "    return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))\n",
    "\n",
    "\n",
    "# Initialize all layers for a fully-connected neural network with sizes \"sizes\"\n",
    "def init_network_params(sizes, key):\n",
    "    keys = random.split(key, len(sizes))\n",
    "    return [\n",
    "        random_layer_params(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)\n",
    "    ]\n",
    "\n",
    "\n",
    "layer_sizes = [784, 512, 512, 10]\n",
    "step_size = 0.01\n",
    "num_epochs = 8\n",
    "batch_size = 128\n",
    "n_targets = 10\n",
    "params = init_network_params(layer_sizes, random.key(0))\n",
    "\n",
    "## Auto-batching predictions\n",
    "from jax.scipy.special import logsumexp\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return jnp.maximum(0, x)\n",
    "\n",
    "\n",
    "def predict(params, image):\n",
    "    # per-example predictions\n",
    "    activations = image\n",
    "    for w, b in params[:-1]:\n",
    "        outputs = jnp.dot(w, activations) + b\n",
    "        activations = relu(outputs)\n",
    "\n",
    "    final_w, final_b = params[-1]\n",
    "    logits = jnp.dot(final_w, activations) + final_b\n",
    "    return logits - logsumexp(logits)\n",
    "\n",
    "\n",
    "# This works on single examples. Let's upgrade it to handle batches using `vmap`\n",
    "batched_predict = vmap(predict, in_axes=(None, 0))\n",
    "\n",
    "\n",
    "## Utility and loss functions\n",
    "def one_hot(x, k, dtype=jnp.float32):\n",
    "    \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
    "    return jnp.array(x[:, None] == jnp.arange(k), dtype)\n",
    "\n",
    "\n",
    "def accuracy(params, images, targets):\n",
    "    target_class = jnp.argmax(targets, axis=1)\n",
    "    predicted_class = jnp.argmax(batched_predict(params, images), axis=1)\n",
    "    return jnp.mean(predicted_class == target_class)\n",
    "\n",
    "\n",
    "def loss(params, images, targets):\n",
    "    preds = batched_predict(params, images)\n",
    "    return -jnp.mean(preds * targets)\n",
    "\n",
    "\n",
    "@jit\n",
    "def update(params, x, y):\n",
    "    grads = grad(loss)(params, x, y)\n",
    "    return [\n",
    "        (w - step_size * dw, b - step_size * db)\n",
    "        for (w, b), (dw, db) in zip(params, grads)\n",
    "    ]\n",
    "\n",
    "\n",
    "## Data Loading with PyTorch\n",
    "import numpy as np\n",
    "from jax.tree_util import tree_map\n",
    "from torch.utils import data\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "def numpy_collate(batch):\n",
    "    return tree_map(np.asarray, data.default_collate(batch))\n",
    "\n",
    "\n",
    "class NumpyLoader(data.DataLoader):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        sampler=None,\n",
    "        batch_sampler=None,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "        timeout=0,\n",
    "        worker_init_fn=None,\n",
    "    ):\n",
    "        super(self.__class__, self).__init__(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            sampler=sampler,\n",
    "            batch_sampler=batch_sampler,\n",
    "            num_workers=num_workers,\n",
    "            collate_fn=numpy_collate,\n",
    "            pin_memory=pin_memory,\n",
    "            drop_last=drop_last,\n",
    "            timeout=timeout,\n",
    "            worker_init_fn=worker_init_fn,\n",
    "        )\n",
    "\n",
    "\n",
    "class FlattenAndCast(object):\n",
    "    def __call__(self, pic):\n",
    "        return np.ravel(np.array(pic, dtype=jnp.float32))\n",
    "\n",
    "\n",
    "# Define our dataset, using torch datasets\n",
    "mnist_dataset = MNIST(\"./\", download=False, transform=FlattenAndCast(), train=True)\n",
    "training_generator = NumpyLoader(mnist_dataset, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "# Get the full train dataset (for checking accuracy while training)\n",
    "train_images = np.array(mnist_dataset.train_data).reshape(\n",
    "    len(mnist_dataset.train_data), -1\n",
    ")\n",
    "train_labels = one_hot(np.array(mnist_dataset.train_labels), n_targets)\n",
    "\n",
    "# Get full test dataset\n",
    "mnist_dataset_test = MNIST(\n",
    "    \"./\",\n",
    "    download=False,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    train=False,\n",
    ")\n",
    "test_images = jnp.array(\n",
    "    mnist_dataset_test.test_data.numpy().reshape(len(mnist_dataset_test.test_data), -1),\n",
    "    dtype=jnp.float32,\n",
    ")\n",
    "test_labels = one_hot(np.array(mnist_dataset_test.test_labels), n_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training loop\n",
    "import time\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    for x, y in training_generator:\n",
    "        y = one_hot(y, n_targets)\n",
    "        params = update(params, x, y)\n",
    "    epoch_time = time.time() - start_time\n",
    "\n",
    "    train_acc = accuracy(params, train_images, train_labels)\n",
    "    test_acc = accuracy(params, test_images, test_labels)\n",
    "    print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n",
    "    print(\"Training set accuracy {}\".format(train_acc))\n",
    "    print(\"Test set accuracy {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize some of the images and their predictions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_images_with_labels_and_pred(images, labels, preds):\n",
    "    _, axs = plt.subplots(2, 5, figsize=(10, 4))\n",
    "    axs = axs.ravel()\n",
    "    for i in range(10):\n",
    "        axs[i].imshow(images[i].reshape(28, 28), cmap=\"gray\")\n",
    "        axs[i].set_title(\"Label: {}\\nPred: {}\".format(labels[i], preds[i]))\n",
    "        axs[i].axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot the first 10 test images\n",
    "plot_images_with_labels_and_pred(\n",
    "    test_images[:10],\n",
    "    np.argmax(test_labels[:10], axis=1),\n",
    "    np.argmax(batched_predict(params, test_images[:10]), axis=1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will adapt this neural network to see it as a generative function in Genjax.\n",
    "To do so, we will define a noisy renderer from which the images from MNIST dataset will be observations.\n",
    "The goal of the inference task for the model is to infer what the label is (i.e. which digit) for a given observation (ofi.e. image), and we will tune the parameters generative function to do a better job on average on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import genjax\n",
    "from genjax import gen, normal\n",
    "from jax import jit\n",
    "\n",
    "\n",
    "# First, we define a noisy rendering function\n",
    "# This is an extremely simple model that adds Gaussian noise to each pixel\n",
    "@gen\n",
    "def pixel_renderer(pixel):\n",
    "    noisy_pixel = normal(pixel, 50.0) @ \"noisy_pixel\"\n",
    "    return noisy_pixel\n",
    "\n",
    "\n",
    "image_renderer = jit(\n",
    "    genjax.vmap(in_axes=(0,))(genjax.vmap(in_axes=(0,))(pixel_renderer)).simulate\n",
    ")\n",
    "\n",
    "# Let's generate some noisy images and plot them\n",
    "key = random.PRNGKey(0)\n",
    "keys = random.split(key, 5)\n",
    "\n",
    "\n",
    "def plot_images_and_noisy(images, noisy_images):\n",
    "    n = len(images)\n",
    "    _, axs = plt.subplots(2, n, figsize=(10, 4))\n",
    "    axs = axs.ravel()\n",
    "    for i in range(n):\n",
    "        axs[i].imshow(images[i].reshape(28, 28), cmap=\"gray\")\n",
    "        axs[i].set_title(\"Original Image\")\n",
    "        axs[i].axis(\"off\")\n",
    "    for i in range(n):\n",
    "        axs[i + n].imshow(noisy_images[i].reshape(28, 28), cmap=\"gray\")\n",
    "        axs[i + n].set_title(\"Noisy Image\")\n",
    "        axs[i + n].axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "images = test_images[:5]\n",
    "noisy_images = list(\n",
    "    map(lambda x, y: image_renderer(y, (x.reshape(28, 28),)).get_retval(), images, keys)\n",
    ")\n",
    "noisy_images\n",
    "plot_images_and_noisy(images, noisy_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we define a simple generative model that takes in a latent vector of parameters and generates an image.\n",
    "# We will rewrite a more efficient batched version later.\n",
    "\n",
    "from genjax import categorical\n",
    "from jax import nn\n",
    "\n",
    "# Hyperparameters\n",
    "num_digits = 10\n",
    "layer_sizes = [10, 512, 512, 784]\n",
    "# Initialize the network parameters\n",
    "key = random.PRNGKey(0)\n",
    "params = init_network_params(layer_sizes, key)\n",
    "noise_params = [random.normal(key, (784,)) for key in random.split(key, 784)]\n",
    "\n",
    "\n",
    "# Utility functions\n",
    "def create_one_hot_vector(integer, size):\n",
    "    return nn.one_hot(jnp.array(integer), size, axis=-1)[0]\n",
    "\n",
    "\n",
    "# Keeping the image as a 1D array to match the MNIST dataset\n",
    "oneD_noisy_renderer = genjax.vmap(in_axes=(0,))(pixel_renderer)\n",
    "\n",
    "\n",
    "# TODO: Adapt to use noise_params for variance of the Gaussian noise\n",
    "@gen\n",
    "def model(params, noise_params):\n",
    "    prior = jnp.ones(num_digits) / num_digits\n",
    "    digit = categorical(prior) @ \"digit\"\n",
    "    activations = create_one_hot_vector((digit,), num_digits)\n",
    "\n",
    "    for w, b in params:\n",
    "        outputs = jnp.dot(w, activations) + b\n",
    "        activations = relu(outputs)\n",
    "\n",
    "    noisy_image = oneD_noisy_renderer((activations,)) @ \"noisy_image\"\n",
    "    return noisy_image\n",
    "\n",
    "\n",
    "jitted_model = jit(model.simulate)\n",
    "trace = jitted_model(key, (params, noise_params))\n",
    "noisy_image = trace.get_retval()\n",
    "\n",
    "\n",
    "# Version to print the non-noisy image\n",
    "@gen\n",
    "def image_from_digit(params):\n",
    "    prior = jnp.ones(num_digits) / num_digits\n",
    "    digit = categorical(prior) @ \"digit\"\n",
    "    activations = create_one_hot_vector((digit,), num_digits)\n",
    "\n",
    "    for w, b in params:\n",
    "        outputs = jnp.dot(w, activations) + b\n",
    "        activations = relu(outputs)\n",
    "\n",
    "    return activations\n",
    "\n",
    "\n",
    "# We can test the untrained generative model by sampling from it\n",
    "jitted_image_from_digit = jit(image_from_digit.simulate)\n",
    "tr = jitted_image_from_digit(key, (params,))\n",
    "image = tr.get_retval()\n",
    "\n",
    "plot_images_and_noisy(\n",
    "    jnp.expand_dims(image, axis=0), jnp.expand_dims(noisy_image, axis=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can do inference on the untrained model for a fixed image by using importance sampling\n",
    "\n",
    "# Create an observation by creating a partial choice map with a fixed image\n",
    "from genjax import ChoiceMapBuilder as C\n",
    "from genjax import Target, smc\n",
    "\n",
    "chm = C[\"noisy_image\"].set(test_images[5])\n",
    "\n",
    "# Run importance sampling, once\n",
    "arg = (params, noise_params)\n",
    "key = random.PRNGKey(0)\n",
    "tr, _ = jit(model.importance)(key, chm, arg)\n",
    "print(\"infered digit:\", tr.get_sample()[\"digit\"])\n",
    "\n",
    "\n",
    "# Util to show the posterior on digits\n",
    "def plot_posterior(digits, weights):\n",
    "    n = len(digits)\n",
    "    d = dict()\n",
    "    for i in range(n):\n",
    "        element = digits[i]\n",
    "        if element in d:\n",
    "            d[element] += weights[i]\n",
    "        else:\n",
    "            d[element] = weights[i]\n",
    "    sorted_d = {key: d[key] for key in sorted(d.keys())}\n",
    "    return sorted_d\n",
    "\n",
    "\n",
    "# Run importance sampling, multiple times\n",
    "# TODO: is currently ignoring the weight from IS, need normalization\n",
    "sub_keys = random.split(key, 1000)\n",
    "tr = jit(vmap(model.simulate, in_axes=(0, None)))(sub_keys, arg)\n",
    "digits = tr.get_sample()[\"digit\"]\n",
    "weights = tr.get_score()\n",
    "print(\"Approximate posterior:\", plot_posterior(np.array(digits), weights))\n",
    "\n",
    "# TODO: alternative way to run importance sampling\n",
    "# target = Target(model, (arg,), chm)\n",
    "# alg = smc.ImportanceK(target, k_particles=500)\n",
    "# sub_keys = random.split(key, 50)\n",
    "# posterior_samples = jit(vmap(alg.simulate, in_axes=(0, None)))(\n",
    "#     sub_keys, (target,)\n",
    "# ).get_retval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import genjax\n",
    "from genjax._src.adev.core import (\n",
    "    expectation,\n",
    ")\n",
    "\n",
    "from genjax._src.core.typing import (\n",
    "    PRNGKey,\n",
    "    Tuple,\n",
    ")\n",
    "\n",
    "# Next, we can train the model by improving the parameters in a variety of ways.\n",
    "# One way is to define a loss function that measures the difference between the generated image and the observed image.\n",
    "\n",
    "# A simple loss function on images is the mean squared error\n",
    "# def loss(image_1, image_2):\n",
    "#     return jnp.mean((image_1 - image_2) ** 2)\n",
    "\n",
    "# The total loss function we want to minimize is given by\n",
    "# E_{img ~ Uniform(MNIST)} E_{img' ~ model} [loss(img, img')]\n",
    "# i.e. we train the parameters to minimize the expected loss between the generated images and the observed images, averaged over all images in the MNIST dataset.\n",
    "# This is an instance of amortized inference, where we learn a generative model that can generate images that are close to the observed images.\n",
    "# Reverting the process by observing an image, we can infer the posterior distribution on digits that could generate the image under the model.\n",
    "\n",
    "# To do so, we will need to differentiate through the model and the loss function. We will use ADEV to do this.\n",
    "# We have to slightly rewrite the model to be compatible with ADEV.\n",
    "\n",
    "num_digits = 10\n",
    "layer_sizes = [10, 512, 512, 784]\n",
    "# TODO: generation is harder than classification, won't work with such a simple dense network.\n",
    "params = init_network_params(layer_sizes, key)\n",
    "\n",
    "\n",
    "@gen\n",
    "def diff_pixel_renderer(pixel):\n",
    "    noisy_pixel = (\n",
    "        genjax.vi.normal_reparam(pixel, 50.0) @ \"noisy_pixel\"\n",
    "    )  # Note that it's the only change!\n",
    "    return noisy_pixel\n",
    "\n",
    "\n",
    "diff_oneD_noisy_renderer = genjax.vmap(in_axes=(0,))(diff_pixel_renderer)\n",
    "\n",
    "\n",
    "@gen\n",
    "def diff_model(params):\n",
    "    prior = jnp.ones(num_digits) / num_digits\n",
    "    digit = categorical(prior) @ \"digit\"\n",
    "    activations = create_one_hot_vector((digit,), num_digits)\n",
    "\n",
    "    for w, b in params:\n",
    "        outputs = jnp.dot(w, activations) + b\n",
    "        activations = relu(outputs)\n",
    "\n",
    "    noisy_image = diff_oneD_noisy_renderer((activations,)) @ \"noisy_image\"\n",
    "    return noisy_image\n",
    "\n",
    "\n",
    "# Testing\n",
    "jitted_diff_model = jit(diff_model.simulate)\n",
    "params = init_network_params(layer_sizes, key)\n",
    "trace = jitted_diff_model(key, (params,))\n",
    "noisy_image = trace.get_sample()[\"noisy_image\", ..., \"noisy_pixel\"]\n",
    "\n",
    "plot_images_and_noisy(\n",
    "    jnp.expand_dims(image, axis=0), jnp.expand_dims(noisy_image, axis=0)\n",
    ")\n",
    "\n",
    "\n",
    "# Gradient estimate function\n",
    "def grad_estimate(\n",
    "    key: PRNGKey,\n",
    "    args: Tuple,\n",
    ") -> Tuple:\n",
    "    @expectation\n",
    "    def loss(*target_args):\n",
    "        first_args = target_args[:-1]\n",
    "        image = target_args[-1]\n",
    "        # first_args = tuple(first_args)\n",
    "        noisy_image = diff_model.simulate(key, (first_args,)).get_retval()\n",
    "        return jnp.mean((noisy_image - image) ** 2)\n",
    "\n",
    "    return loss.grad_estimate(key, args)\n",
    "    # def loss(*target_args):\n",
    "    #     noisy_image = diff_model.simulate(key, *target_args).get_retval()\n",
    "    #     return jnp.mean((noisy_image - test_images[5]) ** 2)\n",
    "    # return loss.estimate(key, args)\n",
    "\n",
    "\n",
    "# Update the model parameters\n",
    "@jit\n",
    "def update(params, params_grad):\n",
    "    return [\n",
    "        (w - step_size * dw, b - step_size * db)\n",
    "        for (w, b), (dw, db) in zip(params, params_grad)\n",
    "    ]\n",
    "\n",
    "\n",
    "# Training the model\n",
    "n_trials = 1000\n",
    "step_size = 0.01\n",
    "key = random.PRNGKey(0)\n",
    "# jitted_loss = jit(grad_estimate)\n",
    "jitted_loss = grad_estimate\n",
    "test_image = jnp.ones((784,))\n",
    "params = (init_network_params(layer_sizes, key), test_image)\n",
    "# params = (init_network_params(layer_sizes, key),)\n",
    "params_grad = jitted_loss(key, params)\n",
    "# for _ in range(n_trials):\n",
    "#     #TODO: fix the bug and need to provide minibatch of images\n",
    "#     key, subkey = random.split(key)\n",
    "#     (params_grad,) = jitted_loss(subkey, (params,))\n",
    "#     params =  update(params, params_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can try a few options by doing fake hand-coded gradient descent, then GD, then ADEV, then maybe RWS etc.\n",
    "# TODO: answer question \"How to batch my model?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: scratch code\n",
    "import pytest\n",
    "\n",
    "\n",
    "@genjax.gen\n",
    "def model(v):\n",
    "    mu = genjax.normal(0.0, 10.0) @ \"mu\"\n",
    "    _ = genjax.normal(mu, 0.1) @ \"v\"\n",
    "\n",
    "\n",
    "@genjax.marginal\n",
    "@genjax.gen\n",
    "def guide(target):\n",
    "    (v,) = target.args\n",
    "    _ = genjax.vi.normal_reparam(v, 0.1) @ \"mu\"\n",
    "\n",
    "\n",
    "key = random.PRNGKey(314159)\n",
    "elbo_grad = genjax.vi.ELBO(guide, lambda v: genjax.Target(model, (v,), C[\"v\"].set(3.0)))\n",
    "v = 0.1\n",
    "jitted = jit(elbo_grad)\n",
    "for _ in range(200):\n",
    "    (v_grad,) = jitted(key, (v,))\n",
    "    v -= 1e-3 * v_grad\n",
    "assert v == pytest.approx(3.0, 5e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: scratch code\n",
    "import genjax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from genjax.adev import expectation, normal_reparam\n",
    "\n",
    "\n",
    "@expectation\n",
    "def model(y):\n",
    "    x = jax.vmap(normal_reparam, in_axes=(0, None))(y, 3.0)\n",
    "    return jnp.sum(x) ** 2\n",
    "\n",
    "\n",
    "key = jax.random.PRNGKey(314159)\n",
    "model.grad_estimate(key, (jnp.ones(10, dtype=float),))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
