{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to do my first inference task, how do I do it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do it with importance sampling, which works as follows. We choose a distribution `q` called a proposal that you we will sample from, and we need a distribution `p` of interest, typically representing a posterior from a model having received observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import genjax\n",
    "import jax\n",
    "from jax import jit\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# A simple python version of the algorithm to get the idea\n",
    "def importance_sample(hard, easy):\n",
    "    def _inner(key, hard_args, easy_args):\n",
    "        trace = easy.simulate(key, *easy_args)   # we sample from the easy distribution, the proposal `q`\n",
    "        chm = trace.get_sample()\n",
    "        easy_logpdf = trace.get_score()  # we evaluate the score of the easy distribution q(x)\n",
    "        hard_logpdf, _ = hard.assess(chm, *hard_args) # we evaluate the score of the hard distribution p(x)\n",
    "        importance_weight = hard_logpdf - easy_logpdf\n",
    "        return (trace, importance_weight) \n",
    "        # we return the trace and the importance weight p(x)/q(x).\n",
    "        # the importance weight corrects the bias of the easy distribution\n",
    "        # compared to the hard distribution\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which we can test on a very simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.96138144 ChoiceMap(...)\n"
     ]
    }
   ],
   "source": [
    "complex_distribution = genjax.normal\n",
    "simple_distribution = genjax.normal\n",
    "\n",
    "complex_args = (0.0, 1.0)\n",
    "simple_args = (3.0, 4.0)\n",
    "key = jax.random.PRNGKey(0)\n",
    "sample, importance_weight = jit(importance_sample(\n",
    "    complex_distribution,\n",
    "    simple_distribution))(key, (complex_args,), (simple_args,))\n",
    "print(importance_weight, sample.get_sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Genjax, every generative function comes equipped with a default proposal which we can use for importance sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChoiceMap(choice_map_fn=XorChmFn(c1=ChoiceMap(choice_map_fn=XorChmFn(c1=ChoiceMap(choice_map_fn=EmptyChmFn()), c2=ChoiceMap(choice_map_fn=StaticChmFn(addr='p', c=ChoiceMap(choice_map_fn=ValueChmFn(v=<jax.Array(0., dtype=float32)>)))))), c2=ChoiceMap(choice_map_fn=StaticChmFn(addr='v', c=ChoiceMap(choice_map_fn=ValueChmFn(v=1))))))\n",
      "-0.6931472\n"
     ]
    }
   ],
   "source": [
    "from genjax import beta, bernoulli, gen\n",
    "from genjax import ChoiceMapBuilder as C\n",
    "\n",
    "@gen\n",
    "def beta_bernoulli_process(u):\n",
    "    p = beta(0.0, u) @ \"p\"\n",
    "    v = bernoulli(p) @ \"v\"\n",
    "    return v\n",
    "\n",
    "obs = C[\"v\"].set(1)\n",
    "args = (0.5,)\n",
    "# The generative function with observation `obs` specifying the value of certain values of the choicemap represent a potentially complex posterior distribution\n",
    "# The method .importance defines a default proposal based on the generative function which targets the posterior distribution\n",
    "trace, weight = beta_bernoulli_process.importance(key, obs, args) # Runs importance sampling once\n",
    "\n",
    "# This returns a pair containing the new trace and the log probability of produced trace under the model\n",
    "print(trace.get_sample())\n",
    "print(weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can also run it in parallel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChoiceMap(choice_map_fn=ValueChmFn(v=<jax.Array float32(100,) ≈3.2 ±3.9 [≥-5.5, ≤1.3e+01] nonzero:100>)) [-3.4747601e+00 -5.6454945e-01  2.6851845e-01 -2.2750998e-01\n",
      "  1.5283911e+00  1.1302544e+00 -1.3746918e+01  1.1515448e+00\n",
      " -1.4873056e+01 -6.5011601e+00 -2.7984755e+00  1.2259059e+00\n",
      "  1.5403748e-01 -4.1239843e+00 -1.2379522e+00 -2.2136116e-01\n",
      " -1.6777248e+00 -7.1614046e+00  1.6447321e+00 -3.4785581e+00\n",
      " -9.6021223e+00  1.5933621e+00 -1.7278156e+00 -1.7536148e+01\n",
      "  1.2507987e+00 -1.4664571e+01 -1.8105919e+01 -2.3082194e+00\n",
      "  1.2504525e+00  1.3835938e+00 -1.7943430e+01 -1.0664184e+01\n",
      " -5.0033302e+00 -3.1931292e+01 -1.6972691e+01 -2.2477352e+01\n",
      " -7.6195269e+00 -1.6014793e+01 -1.0731525e+01 -5.4594707e-01\n",
      "  1.6281046e+00 -5.4090424e+01 -5.6088039e+01 -2.0370581e+00\n",
      " -1.7183456e+00 -1.2481325e+00 -5.3300409e+00  7.4058771e-02\n",
      "  1.6834406e+00 -2.2230260e+00 -3.9014757e+00  1.3088427e+00\n",
      " -7.6998377e-01 -7.4338651e-01  7.2120810e-01 -4.8259357e+01\n",
      " -4.9510094e+01  9.9923849e-02 -3.1988434e+01 -4.4970789e+00\n",
      " -1.8315599e+01 -5.2054296e+00 -1.2762682e+01 -5.6685181e+00\n",
      " -7.7529368e+00 -1.1329316e+01  8.5557890e-01 -3.5835972e+01\n",
      " -2.8423229e+01 -5.6312332e+01 -1.2532660e+01 -1.2875018e+00\n",
      " -9.8406448e+00  1.4250853e+00 -2.7232693e+01  1.6720717e+00\n",
      " -2.2762159e+01  1.6737509e+00  1.6593003e+00 -3.8651279e+01\n",
      " -4.1997200e+01 -1.4477625e+01 -2.5967644e+01  1.5838397e+00\n",
      " -2.4841383e+00 -7.9074430e+00 -3.7971554e+00  1.0076019e+00\n",
      " -1.6447819e+01 -2.4555450e+01 -1.1647556e+01 -4.2743755e+01\n",
      "  1.0553932e+00  9.7918510e-03  1.6492856e+00 -4.5937939e+00\n",
      "  1.6737807e+00 -1.0123281e+00 -7.5223701e+01 -2.0933771e+01]\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "jitted = jax.jit(jax.vmap(importance_sample(complex_distribution, simple_distribution), in_axes=(0, None, None)))\n",
    "key, *sub_keys = jax.random.split(key, 100 + 1)\n",
    "sub_keys = jnp.array(sub_keys)\n",
    "(sample, importance_weight) = jitted(sub_keys, (complex_args,), (simple_args,))\n",
    "print(sample.get_choices(), importance_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert `N` weighted samples from importance sampling to `K` non-weighted samples that approximate the posterior.\n",
    "This is K-sample importance resample or K-SIR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChoiceMap(choice_map_fn=XorChmFn(c1=ChoiceMap(choice_map_fn=XorChmFn(c1=ChoiceMap(choice_map_fn=EmptyChmFn()), c2=ChoiceMap(choice_map_fn=EmptyChmFn()))), c2=ChoiceMap(choice_map_fn=EmptyChmFn())))\n"
     ]
    }
   ],
   "source": [
    "N = 1000\n",
    "K = 100\n",
    "\n",
    "def sir(N,K,dist,chm):\n",
    "    def _inner(key, args):\n",
    "        key, subkey = jax.random.split(key, 2)\n",
    "        samples, weights = jax.vmap(dist.importance, in_axes=(0, None, None))(jax.random.split(key, N), chm, args)\n",
    "        \n",
    "        idx = jax.vmap(jax.jit(genjax.categorical.simulate), in_axes=(0, None))(\n",
    "            jax.random.split(subkey, K), (weights,)).get_retval()\n",
    "\n",
    "        choicemap = samples.get_choices()\n",
    "        final_samples = jax.tree.map(lambda x: choicemap(x), idx)\n",
    "        return final_samples\n",
    "    return _inner\n",
    "\n",
    "# Testing\n",
    "key = jax.random.PRNGKey(0)\n",
    "chm = C[\"v\"].set(1)\n",
    "args = (0.5,)\n",
    "samples = jit(sir(N,K,beta_bernoulli_process, chm))(key, args)\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChoiceMap(choice_map_fn=FilteredChmFn(selection=Selection(selection_function=CompSelFn(s=Selection(selection_function=ChmSelFn(c=ChoiceMap(choice_map_fn=StaticChmFn(addr='v', c=ChoiceMap(choice_map_fn=ValueChmFn(v=<jax.Array int32(100,) [≥1, ≤1] nonzero:100>)))))))), c=ChoiceMap(choice_map_fn=XorChmFn(c1=ChoiceMap(choice_map_fn=XorChmFn(c1=ChoiceMap(choice_map_fn=EmptyChmFn()), c2=ChoiceMap(choice_map_fn=StaticChmFn(addr='p', c=ChoiceMap(choice_map_fn=ValueChmFn(v=<jax.Array float32(100, 1000, 1)>)))))), c2=ChoiceMap(choice_map_fn=StaticChmFn(addr='v', c=ChoiceMap(choice_map_fn=ValueChmFn(v=<jax.Array int32(100, 1000)>))))))))\n",
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method ImportanceK.get_num_particles of ImportanceK(target=Target(p=StaticGenerativeFunction(source=Closure(dyn_args=(), fn=<function beta_bernoulli_process at 0x2cc514f40>)), args=((0.5,),), constraint=ChoiceMap(choice_map_fn=StaticChmFn(addr='v', c=ChoiceMap(choice_map_fn=ValueChmFn(v=1))))), q=Const(const=None), k_particles=1000)>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another way to do the basically the same thing using library functions\n",
    "from genjax import Target, smc\n",
    "from jax import random, vmap\n",
    "\n",
    "N = 1000\n",
    "K = 100\n",
    "key = jax.random.PRNGKey(0)\n",
    "chm = C[\"v\"].set(1)\n",
    "arg = (0.5,)\n",
    "target_posterior = Target(beta_bernoulli_process, (arg,), chm) # We define the target distribution, a posterior distribution in this case\n",
    "alg = smc.ImportanceK(target_posterior, k_particles=N) # We specify what inference strategy we want to use, in this case SIR with N particles\n",
    "sub_keys = random.split(key, K) # To get K independent samples from the posterior distribution, i.e. running N-particles based SIR K times.\n",
    "# It's a bit different from the previous example, because each of the final \n",
    "# K samples is obtained by running a different set of N-particles.\n",
    "posterior_samples = jit(vmap(alg.simulate, in_axes=(0, None)))(\n",
    "    sub_keys, (target_posterior,)\n",
    ").get_retval()\n",
    "\n",
    "#TODO: finish below\n",
    "print(posterior_samples)\n",
    "\n",
    "_, p_chm = jax.vmap(alg.random_weighted, in_axes=(0, None))(\n",
    "    sub_keys, target_posterior\n",
    "    )\n",
    "\n",
    "# An estimate of `p` over 50 independent trials of SIR (with K = 50 particles).\n",
    "print(jnp.mean(p_chm[\"p\"]))\n",
    "\n",
    "alg.get_num_particles\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
