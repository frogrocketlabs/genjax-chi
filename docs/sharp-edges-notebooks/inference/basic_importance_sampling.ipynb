{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to do my first inference task, how do I do it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do it with importance sampling, which works as follows. We choose a distribution `q` called a proposal that you we will sample from, and we need a distribution `p` of interest, typically representing a posterior from a model having received observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import genjax\n",
    "import jax\n",
    "from jax import jit\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "# A simple python version of the algorithm to get the idea\n",
    "def importance_sample(hard, easy):\n",
    "    def _inner(key, hard_args, easy_args):\n",
    "        trace = easy.simulate(\n",
    "            key, *easy_args\n",
    "        )  # we sample from the easy distribution, the proposal `q`\n",
    "        chm = trace.get_sample()\n",
    "        easy_logpdf = (\n",
    "            trace.get_score()\n",
    "        )  # we evaluate the score of the easy distribution q(x)\n",
    "        hard_logpdf, _ = hard.assess(\n",
    "            chm, *hard_args\n",
    "        )  # we evaluate the score of the hard distribution p(x)\n",
    "        importance_weight = hard_logpdf - easy_logpdf\n",
    "        return (trace, importance_weight)\n",
    "        # we return the trace and the importance weight p(x)/q(x).\n",
    "        # the importance weight corrects the bias of the easy distribution\n",
    "        # compared to the hard distribution\n",
    "\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which we can test on a very simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_distribution = genjax.normal\n",
    "simple_distribution = genjax.normal\n",
    "\n",
    "complex_args = (0.0, 1.0)\n",
    "simple_args = (3.0, 4.0)\n",
    "key = jax.random.PRNGKey(0)\n",
    "sample, importance_weight = jit(\n",
    "    importance_sample(complex_distribution, simple_distribution)\n",
    ")(key, (complex_args,), (simple_args,))\n",
    "print(importance_weight, sample.get_sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Genjax, every generative function comes equipped with a default proposal which we can use for importance sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genjax import beta, bernoulli, gen\n",
    "from genjax import ChoiceMapBuilder as C\n",
    "\n",
    "\n",
    "@gen\n",
    "def beta_bernoulli_process(u):\n",
    "    p = beta(0.0, u) @ \"p\"\n",
    "    v = bernoulli(p) @ \"v\"\n",
    "    return v\n",
    "\n",
    "\n",
    "obs = C[\"v\"].set(1)\n",
    "args = (0.5,)\n",
    "# The generative function with observation `obs` specifying the value of certain values of the choicemap represent a potentially complex posterior distribution\n",
    "# The method .importance defines a default proposal based on the generative function which targets the posterior distribution\n",
    "trace, weight = beta_bernoulli_process.importance(\n",
    "    key, obs, args\n",
    ")  # Runs importance sampling once\n",
    "\n",
    "# This returns a pair containing the new trace and the log probability of produced trace under the model\n",
    "print(trace.get_sample())\n",
    "print(weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can also run it in parallel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "jitted = jax.jit(\n",
    "    jax.vmap(\n",
    "        importance_sample(complex_distribution, simple_distribution),\n",
    "        in_axes=(0, None, None),\n",
    "    )\n",
    ")\n",
    "key, *sub_keys = jax.random.split(key, 100 + 1)\n",
    "sub_keys = jnp.array(sub_keys)\n",
    "(sample, importance_weight) = jitted(sub_keys, (complex_args,), (simple_args,))\n",
    "print(sample.get_choices(), importance_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert `N` weighted samples from importance sampling to `K` non-weighted samples that approximate the posterior.\n",
    "This is K-sample importance resample or K-SIR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "K = 100\n",
    "\n",
    "\n",
    "def sir(N, K, dist, chm):\n",
    "    def _inner(key, args):\n",
    "        key, subkey = jax.random.split(key, 2)\n",
    "        samples, weights = jax.vmap(dist.importance, in_axes=(0, None, None))(\n",
    "            jax.random.split(key, N), chm, args\n",
    "        )\n",
    "\n",
    "        idx = jax.vmap(jax.jit(genjax.categorical.simulate), in_axes=(0, None))(\n",
    "            jax.random.split(subkey, K), (weights,)\n",
    "        ).get_retval()\n",
    "\n",
    "        choicemap = samples.get_choices()\n",
    "        final_samples = jax.tree.map(lambda x: choicemap(x), idx)\n",
    "        return final_samples\n",
    "\n",
    "    return _inner\n",
    "\n",
    "\n",
    "# Testing\n",
    "key = jax.random.PRNGKey(0)\n",
    "chm = C[\"v\"].set(1)\n",
    "args = (0.5,)\n",
    "samples = jit(sir(N, K, beta_bernoulli_process, chm))(key, args)\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to do the basically the same thing using library functions\n",
    "from genjax import Target, smc\n",
    "from jax import random, vmap\n",
    "\n",
    "N = 1000\n",
    "K = 100\n",
    "key = jax.random.PRNGKey(0)\n",
    "chm = C[\"v\"].set(1)\n",
    "arg = (0.5,)\n",
    "target_posterior = Target(\n",
    "    beta_bernoulli_process, (arg,), chm\n",
    ")  # We define the target distribution, a posterior distribution in this case\n",
    "alg = smc.ImportanceK(\n",
    "    target_posterior, k_particles=N\n",
    ")  # We specify what inference strategy we want to use, in this case SIR with N particles\n",
    "sub_keys = random.split(\n",
    "    key, K\n",
    ")  # To get K independent samples from the posterior distribution, i.e. running N-particles based SIR K times.\n",
    "# It's a bit different from the previous example, because each of the final\n",
    "# K samples is obtained by running a different set of N-particles.\n",
    "posterior_samples = jit(vmap(alg.simulate, in_axes=(0, None)))(\n",
    "    sub_keys, (target_posterior,)\n",
    ").get_retval()\n",
    "\n",
    "# TODO: finish below\n",
    "print(posterior_samples)\n",
    "\n",
    "_, p_chm = jax.vmap(alg.random_weighted, in_axes=(0, None))(sub_keys, target_posterior)\n",
    "\n",
    "# An estimate of `p` over 50 independent trials of SIR (with K = 50 particles).\n",
    "print(jnp.mean(p_chm[\"p\"]))\n",
    "\n",
    "alg.get_num_particles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
