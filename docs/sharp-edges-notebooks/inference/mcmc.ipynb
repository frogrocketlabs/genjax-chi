{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is MCMC? How do I use it? How do I write one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "from genjax.inference.smc import *\n",
    "from genjax import ChoiceMapBuilder as C\n",
    "from genjax._src.core.interpreters.incremental import Diff\n",
    "from genjax import gen, GenericProblem, normal\n",
    "from jax import jit\n",
    "\n",
    "\n",
    "# We can first define a simple model using GenJAX.\n",
    "@gen\n",
    "def model(x):\n",
    "    a = normal(0.0, 5.0) @ \"a\"\n",
    "    b = normal(0.0, 1.0) @ \"b\"\n",
    "    y = normal(a * x + b, 1.0) @ \"y\"\n",
    "    return y\n",
    "\n",
    "\n",
    "# Together with observations, this creates a posterior inference problem.\n",
    "obs = C[\"y\"].set(4.0)\n",
    "\n",
    "\n",
    "# The key ingredient in MCMC is a transition kernel.\n",
    "# We can write it in GenJAX as a function that takes a current trace and returns a new trace.\n",
    "# Let's write a simple Metropolis-Hastings (MH) kernel.\n",
    "def metropolis_hastings_move(mh_args, subkey):\n",
    "    # For now, we give the kernel the full state of the model, the proposal, and the observations.\n",
    "    trace, model, proposal, proposal_args, observations = mh_args\n",
    "    model_args = trace.get_args()\n",
    "\n",
    "    # The core computation is updating a trace, and for that we will call the model's update method.\n",
    "    # The update method takes a random key, a trace, and a GenericProblem object.\n",
    "    # The former contains a pair of a Diff object for the arguments and a ChoiceMap of observations.\n",
    "    argdiffs = Diff.tree_diff_no_change(model_args)\n",
    "    proposal_args_forward = (trace, *proposal_args)\n",
    "\n",
    "    # We sample the proposed changes to the trace.\n",
    "    # This is encapsulated in a simple GenJAX generative function.\n",
    "    fwd_choices, fwd_weight, _ = proposal.propose(subkey, proposal_args_forward)\n",
    "\n",
    "    update = GenericProblem(argdiffs, fwd_choices)\n",
    "    new_trace, weight, _, discard = model.update(subkey, trace, update)\n",
    "\n",
    "    # Because we are using MH, we don't directly accept the new trace.\n",
    "    # Instead, we compute a (log) acceptance ratio α and decide whether to accept the new trace, and otherwise keep the old one.\n",
    "    proposal_args_backward = (new_trace, *proposal_args)\n",
    "    bwd_weight, _ = proposal.assess(discard, proposal_args_backward)\n",
    "    α = weight - fwd_weight + bwd_weight\n",
    "    ret_fun = jax.lax.cond(\n",
    "        jnp.log(jax.random.uniform(subkey)) < α, lambda: new_trace, lambda: trace\n",
    "    )\n",
    "    return (ret_fun, model, proposal, proposal_args, observations), ret_fun\n",
    "\n",
    "\n",
    "# We define a simple proposal distribution for the changes in the trace using a Gaussian drift around the current value of \"a\".\n",
    "@gen\n",
    "def prop(tr, *_):\n",
    "    orig_a = tr.get_choices()[\"a\"]\n",
    "    a = normal(orig_a, 1.0) @ \"a\"\n",
    "    return a\n",
    "\n",
    "\n",
    "# The overall MH algorithm is a loop that repeatedly applies the MH kernel,\n",
    "# which can conveniently be written using jax.lax.scan.\n",
    "def mh(trace, model, proposal, proposal_args, observations, key, num_updates):\n",
    "    mh_keys = jax.random.split(key, num_updates)\n",
    "    last_carry, mh_chain = jax.lax.scan(\n",
    "        metropolis_hastings_move,\n",
    "        (trace, model, proposal, proposal_args, observations),\n",
    "        mh_keys,\n",
    "    )\n",
    "    return last_carry[0], mh_chain\n",
    "\n",
    "\n",
    "# Our custom MH algorithm is a simple wrapper around the MH kernel using our chosen proposal distribution.\n",
    "def custom_mh(trace, model, observations, key, num_updates):\n",
    "    return mh(trace, model, prop, (), observations, key, num_updates)\n",
    "\n",
    "\n",
    "# We now want to create a function run_inference that takes the inference problem, i.e. the model and observations, a random key, and returns traces from the posterior.\n",
    "def run_inference(model, model_args, obs, key, num_samples):\n",
    "    key, subkey1, subkey2 = jax.random.split(key, 3)\n",
    "    # TODO: is that ok to ignore the weight?\n",
    "    # We sample once from a default importance sampler to get an initial trace.\n",
    "    tr, _ = model.importance(subkey1, obs, model_args)\n",
    "    # We then run our custom Metropolis-Hastings kernel to rejuvenate the trace.\n",
    "    rejuvenated_trace, mh_chain = custom_mh(tr, model, obs, subkey2, num_samples)\n",
    "    return rejuvenated_trace, mh_chain\n",
    "\n",
    "\n",
    "# We add a little visualization function to validate the results.\n",
    "def validate_mh(mh_chain):\n",
    "    a = mh_chain.get_choices()[\"a\"]\n",
    "    b = mh_chain.get_choices()[\"b\"]\n",
    "    y = mh_chain.get_retval()\n",
    "    x = mh_chain.get_args()[0]\n",
    "    plt.plot(range(len(y)), a * x + b)\n",
    "    plt.plot(range(len(y)), y, color=\"k\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Testing the inference function.\n",
    "key = jax.random.PRNGKey(5000)\n",
    "model_args = (5.0,)\n",
    "num_samples = 40000\n",
    "_, mh_chain = run_inference(model, model_args, obs, key, num_samples)\n",
    "validate_mh(mh_chain)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
