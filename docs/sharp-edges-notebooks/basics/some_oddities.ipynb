{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are some sharp edges of Jax that are good to know before really starting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genjax import bernoulli, gen\n",
    "import jax\n",
    "\n",
    "# 1] Jax expects arrays/tuples everywhere\n",
    "\n",
    "\n",
    "@gen\n",
    "def f(p):\n",
    "    v = bernoulli(p) @ \"v\"\n",
    "    return v\n",
    "\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "# First way of failing\n",
    "try:\n",
    "    f.simulate(key, 0.5)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# Second way of failing\n",
    "try:\n",
    "    f.simulate(key, [0.5])\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# Third way of failing\n",
    "try:\n",
    "    f.simulate(key, (0.5))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# Correct way\n",
    "f.simulate(key, (0.5,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2] We rely on Tensor Flow Probability and it sometimes does weird things.\n",
    "\n",
    "# Bernoulli distribution uses logits instead of probabilities\n",
    "from genjax import bernoulli, gen\n",
    "from genjax import ChoiceMapBuilder as C\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "@gen\n",
    "def g(p):\n",
    "    v = bernoulli(p) @ \"v\"\n",
    "    return v\n",
    "\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "arg = (3.0,)  # 3 is not a valid probability but a valid logit\n",
    "keys = jax.random.split(key, 30)\n",
    "# simulate 30 times\n",
    "print(jnp.array([g.simulate(key, arg).get_sample()[\"v\"] for key in keys]))\n",
    "\n",
    "# Values which are stricter than 0 are considered to be the value True.\n",
    "# This means that observing that the value of \"v\" is 4 will be considered possible while intuitively \"v\" should only have support on 0 and 1.\n",
    "chm = C[\"v\"].set(3)\n",
    "print()\n",
    "print(g.assess(chm, (0.5,))[0])  # This should be -inf.\n",
    "print()\n",
    "\n",
    "# Alternatively, we can use the flip function which uses probabilities instead of logits.\n",
    "from genjax import flip\n",
    "\n",
    "\n",
    "@gen\n",
    "def h(p):\n",
    "    v = flip(p) @ \"v\"\n",
    "    return v\n",
    "\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "arg = (0.3,)  # 0.3 is a valid probability\n",
    "keys = jax.random.split(key, 30)\n",
    "# simulate 30 times\n",
    "print(jnp.array([h.simulate(key, arg).get_sample()[\"v\"] for key in keys]))\n",
    "print()\n",
    "\n",
    "# Categorical distribution also use logits instead of probabilities\n",
    "from genjax import categorical\n",
    "\n",
    "\n",
    "@gen\n",
    "def i(p):\n",
    "    v = categorical(p) @ \"v\"\n",
    "    return v\n",
    "\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "arg = ([3.0, 1.0, 2.0],)  # lists of 3 logits\n",
    "keys = jax.random.split(key, 30)\n",
    "# simulate 30 times\n",
    "print(jnp.array([i.simulate(key, arg).get_sample()[\"v\"] for key in keys]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3] Jax code can be compiled for better performance.\n",
    "from jax import jit\n",
    "\n",
    "\n",
    "# jit is the way to force Jax to compile the code.\n",
    "# It can be used as a decorator\n",
    "@jit\n",
    "def f_v1(p):\n",
    "    jax.lax.cond(p.sum(), lambda p: p * p, lambda p: p * p, p)\n",
    "\n",
    "\n",
    "# Or as a function\n",
    "f_v2 = jit(f_v2)\n",
    "\n",
    "\n",
    "# Baseline\n",
    "def f_v3(p):\n",
    "    jax.lax.cond(p.sum(), lambda p: p * p, lambda p: p * p, p)\n",
    "\n",
    "\n",
    "# Notice that the first and second have the same performance while the third is 10k times slower.\n",
    "arg = jax.numpy.eye(1000)\n",
    "%timeit f_v1(arg)\n",
    "%timeit f_v2(arg)\n",
    "%timeit f_v3(arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4] Going from Python to Jax\n",
    "# For loops\n",
    "def python_loop(x):\n",
    "    for i in range(100):\n",
    "        x = 2 * x\n",
    "    return x\n",
    "\n",
    "\n",
    "def jax_loop(x):\n",
    "    jax.lax.fori_loop(0, 100, lambda i, x: 2 * x, x)\n",
    "\n",
    "\n",
    "# Conditional statements\n",
    "def python_cond(x):\n",
    "    if x.sum() > 0:\n",
    "        return x * x\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "\n",
    "def jax_cond(x):\n",
    "    jax.lax.cond(x.sum(), lambda x: x * x, lambda x: x, x)\n",
    "\n",
    "\n",
    "# While loops\n",
    "def python_while(x):\n",
    "    while x.sum() > 0:\n",
    "        x = x * x\n",
    "    return x\n",
    "\n",
    "\n",
    "def jax_while(x):\n",
    "    jax.lax.while_loop(lambda x: x.sum() > 0, lambda x: x * x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5] In Jax (and GenJax), jit at the outer most level, and in particular not inside a for loop.\n",
    "\n",
    "\n",
    "def innocent_jittable(x):\n",
    "    return jax.lax.fori_loop(0, 100, lambda i, x: 1.1 * x, x)\n",
    "\n",
    "\n",
    "def turtle_compilation_speed(x):\n",
    "    for i in range(100):\n",
    "        x = jit(innocent_jittable)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "%timeit turtle_compilation_speed(1)\n",
    "\n",
    "\n",
    "# the same but non-jitted\n",
    "def human_speed_no_compile(x):\n",
    "    for i in range(100):\n",
    "        x = innocent_jittable(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "%timeit human_speed_no_compile(1.0)\n",
    "\n",
    "# jit at the outer most level\n",
    "jitted = jit(innocent_jittable)\n",
    "\n",
    "\n",
    "def hare_compilation_speed(x):\n",
    "    for i in range(100):\n",
    "        x = jitted(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "# get rid compile time with a first run\n",
    "jitted(1.0)\n",
    "%timeit hare_compilation_speed(1.0)\n",
    "\n",
    "# We can see that the fastest way is to jit the outer most function, which makes it faster than the non-jitted by a factor of !7, and that jitting inside a for loop is an extra 500x slower than the non-jitted version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6] is my thing compiling or is it blocked at traced time?\n",
    "\n",
    "import multiprocessing\n",
    "import time\n",
    "import subprocess\n",
    "\n",
    "# In Jax, the first time you run a function, it is traced, which produces a Jaxpr, a representation of the computation that Jax can optimize.\n",
    "\n",
    "# So in order to debug whether a function is running or not, if it passes the first check that Python let's you write it, you can check if it is running by checking if it is traced, before actually running it on data.\n",
    "\n",
    "\n",
    "# This is done by calling make_jaxpr on the function. If it returns a Jaxpr, then the function is traced and ready to be run on data.\n",
    "def im_fine(x):\n",
    "    return x * x\n",
    "\n",
    "\n",
    "print(jax.make_jaxpr(im_fine)(1.0))\n",
    "print()\n",
    "\n",
    "\n",
    "def i_wont_be_so_fine(x):\n",
    "    return jax.lax.while_loop(lambda x: x > 0, lambda x: x * x, x)\n",
    "\n",
    "\n",
    "print(jax.make_jaxpr(i_wont_be_so_fine)(1.0))\n",
    "print()\n",
    "\n",
    "\n",
    "# Try running the function for 8 seconds\n",
    "def run_process():\n",
    "    ctx = multiprocessing.get_context(\"spawn\")\n",
    "    p = ctx.Process(target=i_wont_be_so_fine, args=(1.0,))\n",
    "    p.start()\n",
    "    time.sleep(8)\n",
    "    if p.is_alive():\n",
    "        print(\"I'm still running\")\n",
    "        p.terminate()\n",
    "        p.join()\n",
    "\n",
    "\n",
    "result = subprocess.run(\n",
    "    [\"python\", \"genjax/docs/sharp-edges-notebooks/basics/script.py\"],\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    ")\n",
    "\n",
    "# Print the output\n",
    "print(result.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7] Using random keys for generative functions\n",
    "\n",
    "# In GenJax, we use explicit random keys to generate random numbers. This is done by splitting a key into multiple keys, and using them to generate random numbers.\n",
    "from genjax import bernoulli, gen, beta\n",
    "\n",
    "\n",
    "@gen\n",
    "def beta_bernoulli_process(u):\n",
    "    p = beta(0.0, u) @ \"p\"\n",
    "    v = bernoulli(p) @ \"v\"  # sweet\n",
    "    return v\n",
    "\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "keys = jax.random.split(key, 20)\n",
    "jitted = jit(beta_bernoulli_process.simulate)\n",
    "print(jnp.array([jitted(key, (0.5,)).get_sample()[\"v\"] for key in keys]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8] Jax uses 32-bit floats by default\n",
    "\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "\n",
    "x = random.uniform(random.key(0), (1000,), dtype=jnp.float64)\n",
    "print(\"surprise surprise: \", x.dtype)\n",
    "print()\n",
    "\n",
    "# A common TypeError occurs when one tries using np instead of jnp, which is the Jax version of numpy, the former uses 64-bit floats by default, while the Jax version uses 32-bit floats by default.\n",
    "\n",
    "# this on its own gives a UserWarning\n",
    "jnp.array([1, 2, 3], dtype=np.float64)\n",
    "\n",
    "# this will truncate the array to 32-bit floats and also give a UserWarning\n",
    "innocent_looking_array = np.array([1.0, 2.0, 3.0], dtype=np.float64)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def innocent_looking_function(x):\n",
    "    return jax.lax.cond(x.sum(), lambda x: x * x, lambda x: innocent_looking_array, x)\n",
    "\n",
    "\n",
    "input = jnp.array([1.0, 2.0, 3.0])\n",
    "innocent_looking_function(input)\n",
    "\n",
    "try:\n",
    "    # This actually raises a TypeError\n",
    "    innocent_looking_array = np.array([1, 2, 3], dtype=np.float64)\n",
    "\n",
    "    @jax.jit\n",
    "    def innocent_looking_function(x):\n",
    "        return jax.lax.cond(\n",
    "            x.sum(), lambda x: x * x, lambda x: innocent_looking_array, x\n",
    "        )\n",
    "\n",
    "    input = jnp.array([1, 2, 3])\n",
    "    innocent_looking_function(input)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9] Beware to OOM on the GPU which happens faster than you might think\n",
    "\n",
    "# Here's a simple HMM model that can be run on the GPU.\n",
    "# By simply changing N from 300 to 1000, the code will typically run out of memory on the GPU as it will take ~300GB of memory.\n",
    "\n",
    "import genjax\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from jax import jit\n",
    "\n",
    "N = 300\n",
    "n_repeats = 100\n",
    "variance = jnp.eye(N)\n",
    "initial_state = jax.random.normal(jax.random.PRNGKey(0), (N,))\n",
    "\n",
    "\n",
    "@genjax.scan_combinator(max_length=100)\n",
    "@genjax.gen\n",
    "def hmm(x, c):\n",
    "    new_x = genjax.mv_normal(x, variance) @ \"new_x\"\n",
    "    return new_x, None\n",
    "\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "key, subkey = jax.random.split(key)\n",
    "jitted = jit(hmm.repeat(num_repeats=n_repeats).simulate)\n",
    "trace = jitted(key, (initial_state, None))\n",
    "%timeit jitted(subkey, (initial_state, None))\n",
    "\n",
    "# If you are running out of memory, you can try de-batching one of the computations, or using a smaller batch size.\n",
    "# For instance, in this example, we can-debatch the repeat combinator, which will reduce the memory usage by a factor of 100, at the cost of some performance.\n",
    "jitted = jit(hmm.simulate)\n",
    "\n",
    "\n",
    "def hmm_debatched(key, initial_state):\n",
    "    keys = jax.random.split(key, n_repeats)\n",
    "    traces = {}\n",
    "    for i in range(n_repeats):\n",
    "        trace = jitted(keys[i], (initial_state, None))\n",
    "        traces[i] = trace\n",
    "    return traces\n",
    "\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "# About 4x slower on arm64 cpu and 40x on a google colab gpu\n",
    "%timeit hmm_debatched(key, initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9] Sometimes tracing can be slow, and you may accelerate it.\n",
    "\n",
    "# TODO: why is nested vmap so slow to trace?\n",
    "# TODO: printing returning traced values or nothing when function is jitted."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
