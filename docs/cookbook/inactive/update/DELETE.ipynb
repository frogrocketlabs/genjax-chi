{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from genjax.delayed import delay, assume, observe\n",
    "from genjax import beta, flip\n",
    "from jax import make_jaxpr, jit, vmap\n",
    "import jax.random as jrand\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"JAX Python\"\n",
    "def fn(x):\n",
    "    v = jnp.sum(x)\n",
    "    z = v + 3.0\n",
    "    return z\n",
    "\n",
    "make_jaxpr(fn)(jnp.ones(5))\n",
    "\n",
    "{ lambda ; a:f32[5]. let\n",
    "    b:f32[] = reduce_sum[axes=(0,)] a\n",
    "    c:f32[] = add b 3.0\n",
    "  in (c,) }\n",
    "\n",
    "# Jaxpr has a set of primitives provided by JAX:\n",
    "# add_p, sub_p, ... all these primitive array operations.\n",
    "# you can add your primitives.\n",
    "\n",
    "# When you add your own primitive -- you need to tell JAX a few things:\n",
    "# * (abstract evaluation) If I give your primitive arrays of this shape and dtype, \n",
    "#   what will you give me back? You have to answer this, if you want your\n",
    "#   primitive to work with `jax.make_jaxpr`\n",
    "# * (batching) How does your primitive work with vmap? If I give you arrays,\n",
    "#   and I tell you that the batching dimension is this, what do you give me back?\n",
    "#   If you answer this, your primitive will work with vmap.\n",
    "\n",
    "# My primitives are never vmapped over -- I always run an interpreter and\n",
    "# eliminate my primitives into pure JAX primitives -- and then vmap works\n",
    "# without a problem.\n",
    "vmap(run_my_interpreter(fn_with_my_primitive))\n",
    "# run_my_interpreter(...) is in \"pure JAX\" (no extension)\n",
    "\n",
    "@gen\n",
    "def model(args):\n",
    "    x = some_other_gen_fn(...) @ \"x\"\n",
    "    y = normal(some_likelihood_jax_computation(x), 3.0) @ \"y\"\n",
    "\n",
    "# introduce a primitive called `trace_p` -- \n",
    "# and then the interpreters in `static.py` eliminate `trace_p` for pure JAX...\n",
    "# model.simulate -> runs an interpreter, which turns primitives into pure JAX\n",
    "vmap(model.simulate)(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "**Marginalizing out the beta in the beta-bernoulli model:**\n",
    "\n",
    "$\\text{flip}(c; p) = p^c (1 - p)^{1 - c}$\n",
    "\n",
    "$\\text{beta}(p; \\alpha, \\beta) = \\frac{1}{B(\\alpha, \\beta)} p^{\\alpha - 1} (1 - p)^{\\beta - 1}$\n",
    "\n",
    "$\\text{marg}(c; \\alpha, \\beta) = \\frac{1}{B(\\alpha, \\beta)} \\int_0^1 p^{\\alpha - 1 + c}(1-p)^{\\beta -c} dp$\n",
    "\n",
    "$\\text{marg}(0; \\alpha, \\beta) = \\frac{1}{B(\\alpha, \\beta)} \\int_0^1 p^{\\alpha - 1}(1-p)^{\\beta} dp = \\frac{B(\\alpha, \\beta + 1)}{B(\\alpha, \\beta)}$\n",
    "\n",
    "$\\text{marg}(1; \\alpha, \\beta) = \\frac{1}{B(\\alpha, \\beta)} \\int_0^1 p^{\\alpha - 1}(1-p)^{\\beta} dp = \\frac{B(\\alpha + 1, \\beta)}{B(\\alpha, \\beta)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low-level target language that supports static delayed sampling.\n",
    "def fn(obs, a, b):\n",
    "    # exact logic\n",
    "    p = assume(beta, a, b)    # p ~ Beta(a, b)\n",
    "    v = observe(obs, flip, p) # observe(obs, Flip(p))\n",
    "    # non-linear with JAX primitives\n",
    "    # ...\n",
    "    return (p, v)\n",
    "\n",
    "# Take a GenFn -> this language\n",
    "# Take a sampler from this language -> GenFn\n",
    "# model.generate(DelayedSampling(\"x\", \"y\"), choice_map({\"z\":3.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen\n",
    "def model():\n",
    "    @gen\n",
    "    def submodel():\n",
    "        p = beta(1.0, 1.0) @ \"p\" # beta.lower(1.0, 1.0) -> assume(beta, 1.0, 1.0) # (\"s1\", \"p\")\n",
    "        return p\n",
    "\n",
    "    @gen\n",
    "    def submodel_(p):\n",
    "        x = flip(p) @ \"f\" # flip.lower(p) -> observe(chm[\"s2\", \"f\"], flip, p) # (\"s2\", \"f\")\n",
    "        return x\n",
    "\n",
    "    # Step 1 in lowering -- submodel.lower() -> Repr\n",
    "    p = submodel() @ \"s1\"\n",
    "    # Step 2 in lowering -- submodel_.lower(p) -> Repr\n",
    "    f = submodel_(p) @ \"s2\"\n",
    "\n",
    "# model.lower(choice_map) -> Repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.generate(DelayedSampling(\"x\", \"y\"), choice_map)\n",
    "# -- Repr: lambda c, a, b:\n",
    "#             %p = assume(beta, a, b)\n",
    "#             observe(c, flip, %p)\n",
    "# \"Use delayed sampling\" -> sample for some subset of the random variables that you care about\n",
    "# Map that sample back into the choice map space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VmapCombinator\n",
    "# s = ScanCombinator(model)\n",
    "# Their logic is kind of complicated -- their \"P\" distribution involves some programmatic control flow dependency stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = ScanCombinator(model :: (C, S1) -> G (C, S2)) :: (C, Vec S1) -> G (C, Vec S2)\n",
    "# s.generate(\"do delayed sampling _within_ model but not between interations\")\n",
    "# s.generate(\"do delayed sampling _across iterations_ of the scan\")\n",
    "# instead of thinking about 1 : N\n",
    "# 1 : 2 -- maybe this will work depending on dependency structure in the scan? \n",
    "# Unroll parts of the scan, and then do delayed sampling on those parts.\n",
    "# Vmap -- each slice is independent of each other -- so you can\n",
    "# Switch -- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def the_actual_beta_posterior_mean(v, N, a, b):\n",
    "    new_a = a + N * v\n",
    "    new_b = b + N - N * v\n",
    "    return new_a / (new_a + new_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "coin = jnp.array(True)\n",
    "a = 2.0\n",
    "b = 2.0\n",
    "\n",
    "# Run a delayed sampler in parallel.\n",
    "jitted = jit(vmap(delay(fn), in_axes=(0, None, None, None)))\n",
    "(p, *_), w = jitted(\n",
    "    jrand.split(jrand.key(3), 10000), \n",
    "    coin, a, b,\n",
    ")\n",
    "jnp.mean(p), the_actual_beta_posterior_mean(coin, 1, a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "(p, p2, *_), w = jitted(\n",
    "    jrand.split(jrand.key(1), 1000), \n",
    "    coin, a, b,\n",
    ")\n",
    "jnp.mean(p), jnp.mean(p2), the_actual_beta_posterior_mean(coin, 1, a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "mjaxpr = make_jaxpr(delay(fn))\n",
    "mjaxpr(\n",
    "    jrand.key(1), \n",
    "    coin, a, b,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
