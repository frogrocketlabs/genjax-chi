{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application: dirichlet mixture model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now see some of the ingredients in action in a simple but more realistic setting and write a dirichlet mixture model in GenJAX."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal here is to cluster datapoints on the real line.\n",
    "To do so, we model a fixed number of clusters, each as a 1D-Gaussian with fixed variance, and we want to infer their means.\n",
    "\n",
    "In more details, the model of the world postulates a fixed number of 1D Gaussians.\n",
    "Each Gaussian is assigned a weight to represent the proportion of the number of points assigned to each cluster.\n",
    "Finally, each datapoint belongs to a cluster, separated proportionally to cluster weights.\n",
    "\n",
    "We turn this into a generative model as follows.\n",
    "We have a fix prior mean and variance for where the clusters centres might be.\n",
    "We sample a mean for each cluster. \n",
    "We sample the cluster weights.\n",
    "For each datapoint, \n",
    "- we sample a cluster assignment for that data point proportional to the cluster weights\n",
    "- we sampled the TODO: add explanation for what's going on and what we will do and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import genjax\n",
    "from genjax import ChoiceMapBuilder as C\n",
    "from genjax import categorical, dirichlet, gen, normal, pretty\n",
    "from genjax._src.core.pytree import Const, Pytree\n",
    "\n",
    "pretty()\n",
    "key = jax.random.key(0)\n",
    "\n",
    "# Hyper parameters\n",
    "PRIOR_MEAN = 10.0\n",
    "PRIOR_VARIANCE = 10.0\n",
    "OBS_VARIANCE = 1.0\n",
    "ALPHA = 1.0\n",
    "N_DATAPOINTS = 10000\n",
    "N_CLUSTERS = 10\n",
    "N_ITER = 1000\n",
    "\n",
    "\n",
    "@Pytree.dataclass\n",
    "class Cluster(Pytree):\n",
    "    mean: float\n",
    "\n",
    "\n",
    "@gen\n",
    "def generate_cluster(mean, var):\n",
    "    cluster_mean = normal(mean, var) @ \"mean\"\n",
    "    return Cluster(cluster_mean)\n",
    "\n",
    "\n",
    "@gen\n",
    "def generate_cluster_weight(alphas):\n",
    "    probs = dirichlet(alphas) @ \"probs\"\n",
    "    return probs\n",
    "\n",
    "\n",
    "@gen\n",
    "def generate_datapoint(probs, clusters):\n",
    "    idx = categorical(jnp.log(probs)) @ \"idx\"\n",
    "    obs = normal(clusters.mean[idx], OBS_VARIANCE) @ \"obs\"\n",
    "    return obs\n",
    "\n",
    "\n",
    "@gen\n",
    "def generate_data(n_clusters: Const[int], n_datapoints: Const[int], alpha: float):\n",
    "    clusters = (\n",
    "        generate_cluster.repeat(n=n_clusters.unwrap())(PRIOR_MEAN, PRIOR_VARIANCE)\n",
    "        @ \"clusters\"\n",
    "    )\n",
    "\n",
    "    probs = generate_cluster_weight.inline(\n",
    "        alpha / n_clusters.unwrap() * jnp.ones(n_clusters.unwrap())\n",
    "    )\n",
    "\n",
    "    datapoints = (\n",
    "        generate_datapoint.repeat(n=n_datapoints.unwrap())(probs, clusters)\n",
    "        @ \"datapoints\"\n",
    "    )\n",
    "\n",
    "    return datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoints = C[\"datapoints\", \"obs\"].set(\n",
    "    jnp.concatenate((\n",
    "        jax.random.uniform(jax.random.key(0), shape=(1000,)),\n",
    "        3 + jax.random.uniform(jax.random.key(1), shape=(1000,)),\n",
    "        5 + jax.random.uniform(jax.random.key(2), shape=(1000,)),\n",
    "        7 + jax.random.uniform(jax.random.key(3), shape=(1000,)),\n",
    "        9 + jax.random.uniform(jax.random.key(4), shape=(1000,)),\n",
    "        11 + jax.random.uniform(jax.random.key(5), shape=(1000,)),\n",
    "        13 + jax.random.uniform(jax.random.key(6), shape=(1000,)),\n",
    "        15 + jax.random.uniform(jax.random.key(7), shape=(1000,)),\n",
    "        17 + jax.random.uniform(jax.random.key(8), shape=(1000,)),\n",
    "        19 + jax.random.uniform(jax.random.key(9), shape=(1000,)),\n",
    "    ))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(datapoints):\n",
    "    key = jax.random.key(3421)\n",
    "    args = (Const(N_CLUSTERS), Const(N_DATAPOINTS), ALPHA)\n",
    "    key, subkey = jax.random.split(key)\n",
    "    tr, _ = generate_data.importance(subkey, datapoints, args)\n",
    "\n",
    "    # TODO: rewrite using scan\n",
    "    # def update(carry, _):\n",
    "    #     key, tr = carry\n",
    "    #     # # Gibbs update on `(\"clusters\", i, \"mean\")` for each i, in parallel\n",
    "    #     # key, subkey = jax.random.split(key)\n",
    "    #     # tr = update_cluster_means(subkey, tr)\n",
    "\n",
    "    #     # Gibbs update on `(\"datapoints\", i, \"idx\")` for each `i`, in parallel\n",
    "    #     key, subkey = jax.random.split(key)\n",
    "    #     tr = update_datapoint_assignment(subkey, tr)\n",
    "\n",
    "    #     # Gibbs update on `probs`\n",
    "    #     key, subkey = jax.random.split(key)\n",
    "    #     tr = update_cluster_weights(subkey, tr)\n",
    "\n",
    "    #     return (tr, key)\n",
    "\n",
    "    # tr, _ = jax.lax.scan(update, (key, tr), None, length=N_ITER)\n",
    "\n",
    "    for _ in range(N_ITER):\n",
    "        # Gibbs update on `(\"clusters\", i, \"mean\")` for each i, in parallel\n",
    "        key, subkey = jax.random.split(key)\n",
    "        tr = jax.jit(update_cluster_means)(subkey, tr)\n",
    "\n",
    "        # Gibbs update on `(\"datapoints\", i, \"idx\")` for each `i`, in parallel\n",
    "        key, subkey = jax.random.split(key)\n",
    "        tr = jax.jit(update_datapoint_assignment)(subkey, tr)\n",
    "\n",
    "        # Gibbs update on `probs`\n",
    "        key, subkey = jax.random.split(key)\n",
    "        tr = jax.jit(update_cluster_weights)(subkey, tr)\n",
    "\n",
    "    return tr\n",
    "\n",
    "\n",
    "def update_cluster_means(key, trace):\n",
    "    # We can update each cluster in parallel\n",
    "    # For each cluster, we find the datapoints in that cluster and compute their mean\n",
    "    datapoint_indexes = trace.get_choices()[\"datapoints\", \"idx\"]\n",
    "    datapoints = trace.get_choices()[\"datapoints\", \"obs\"]\n",
    "    n_clusters = trace.get_args()[0].unwrap()\n",
    "    cluster_means = jax.vmap(\n",
    "        lambda i: jnp.mean(jnp.where(datapoint_indexes == i, datapoints, 0)),\n",
    "        in_axes=(0),\n",
    "        out_axes=(0),\n",
    "    )(jnp.arange(n_clusters))\n",
    "\n",
    "    # Count number of points per cluster\n",
    "    category_counts = jnp.bincount(\n",
    "        trace.get_choices()[\"datapoints\", \"idx\"],\n",
    "        length=n_clusters,\n",
    "        minlength=n_clusters,\n",
    "    )\n",
    "    # Conjugate update for Normal-iid-Normal distribution\n",
    "    # See https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/lectures/lecture5.pdf\n",
    "    posterior_means = (\n",
    "        PRIOR_VARIANCE\n",
    "        / (PRIOR_VARIANCE + OBS_VARIANCE / category_counts)\n",
    "        * cluster_means\n",
    "        + OBS_VARIANCE / (PRIOR_VARIANCE + OBS_VARIANCE / category_counts) * PRIOR_MEAN\n",
    "    )\n",
    "    posterior_variances = 1 / (1 / PRIOR_VARIANCE + category_counts / OBS_VARIANCE)\n",
    "\n",
    "    # Gibbs resampling of cluster means\n",
    "    key, subkey = jax.random.split(key)\n",
    "    new_means = (\n",
    "        generate_cluster.vmap()\n",
    "        .simulate(key, (posterior_means, posterior_variances))\n",
    "        .get_choices()[\"mean\"]\n",
    "    )\n",
    "    argdiffs = genjax.Diff.no_change(trace.args)\n",
    "    new_trace, _, _, _ = trace.update(\n",
    "        subkey, C[\"clusters\", \"mean\"].set(new_means), argdiffs\n",
    "    )\n",
    "    return new_trace\n",
    "\n",
    "\n",
    "def update_datapoint_assignment(key, trace):\n",
    "    # We want to update the index for each datapoint, in parallel.\n",
    "    # It means we want to resample the i, but instead of being from the prior\n",
    "    # P(i | probs), we do it from the local posterior P(i | probs, xs).\n",
    "    # We need to do it for all addresses [\"datapoints\", \"idx\", i],\n",
    "    # and as these are independent (when conditioned on the rest)\n",
    "    # we can resample them in parallel.\n",
    "\n",
    "    # Conjugate update for a categorical is just exact posterior via enumeration\n",
    "    # P(x | y ) = P(x, y) \\ sum_x P(x, y).\n",
    "    # Sampling from Categorical(P(x = 1 | y ), P(x = 2 | y), ...) is the same as\n",
    "    # sampling from Categorical(P(x = 1, y), P(x = 2, y))\n",
    "    # as the weights need not be normalized\n",
    "    def compute_local_density(x, i):\n",
    "        datapoint_mean = trace.get_choices()[\"datapoints\", \"obs\", x]\n",
    "        chm = C[\"obs\"].set(datapoint_mean).at[\"idx\"].set(i)\n",
    "        clusters = Cluster(trace.get_choices()[\"clusters\", \"mean\"])\n",
    "        probs = trace.get_choices()[\"probs\"]\n",
    "        args = (probs, clusters)\n",
    "        model_logpdf, _ = generate_datapoint.assess(chm, args)\n",
    "        return model_logpdf\n",
    "\n",
    "    n_clusters = trace.get_args()[0].unwrap()\n",
    "    n_datapoints = trace.get_args()[1].unwrap()\n",
    "    local_densities = jax.vmap(\n",
    "        lambda x: jax.vmap(lambda i: compute_local_density(x, i))(\n",
    "            jnp.arange(n_clusters)\n",
    "        )\n",
    "    )(jnp.arange(n_datapoints))\n",
    "\n",
    "    # Conjugate update by sampling from posterior categorical\n",
    "    # Note: I think I could've used something like\n",
    "    # generate_datapoint.vmap().importance which would perhaps\n",
    "    # work more generally but would definitely be slower here\n",
    "    key, subkey = jax.random.split(key)\n",
    "    new_datapoint_indexes = (\n",
    "        genjax.categorical.vmap().simulate(key, (local_densities,)).get_choices()\n",
    "    )\n",
    "    # Gibbs resampling of datapoint assignment to clusters\n",
    "    argdiffs = genjax.Diff.no_change(trace.args)\n",
    "    new_trace, _, _, _ = trace.update(\n",
    "        subkey, C[\"datapoints\", \"idx\"].set(new_datapoint_indexes), argdiffs\n",
    "    )\n",
    "    return new_trace\n",
    "\n",
    "\n",
    "def update_cluster_weights(key, trace):\n",
    "    # Count number of points per cluster\n",
    "    n_clusters = trace.get_args()[0].unwrap()\n",
    "    category_counts = jnp.bincount(\n",
    "        trace.get_choices()[\"datapoints\", \"idx\"],\n",
    "        length=n_clusters,\n",
    "        minlength=n_clusters,\n",
    "    )\n",
    "\n",
    "    # Conjugate update for Dirichlet distribution\n",
    "    # See https://en.wikipedia.org/wiki/Dirichlet_distribution#Conjugate_to_categorical_or_multinomial\n",
    "    new_alpha = ALPHA / n_clusters * jnp.ones(n_clusters) + category_counts\n",
    "\n",
    "    # Gibbs resampling of cluster weights\n",
    "    key, subkey = jax.random.split(key)\n",
    "    new_probs = generate_cluster_weight.simulate(key, (new_alpha,)).get_choices()\n",
    "    argdiffs = genjax.Diff.no_change(trace.args)\n",
    "    new_trace, _, _, _ = trace.update(subkey, C[\"probs\"].set(new_probs), argdiffs)\n",
    "    return new_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_trace = infer(datapoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoint = datapoints[\"datapoints\", \"obs\"]\n",
    "posterior_means = posterior_trace.get_choices()[\"clusters\", \"mean\"]\n",
    "posterior_weights = posterior_trace.get_choices()[\"probs\"]\n",
    "\n",
    "# Plotting datapoints\n",
    "plt.scatter(datapoint, jnp.zeros_like(datapoint), alpha=0.1, label=\"Datapoints\", s=20)\n",
    "\n",
    "# Plotting posterior means with size proportional to posterior_weights\n",
    "for i, (mean, weight) in enumerate(zip(posterior_means, posterior_weights)):\n",
    "    plt.scatter(\n",
    "        mean,\n",
    "        0,\n",
    "        color=f\"C{i}\",\n",
    "        label=f\"Cluster {i + 1} Mean (Prob: {weight:.6f})\",\n",
    "        s=100 + weight * 600,\n",
    "        alpha=1,\n",
    "    )\n",
    "\n",
    "# Plotting standard deviation of the Gaussian means\n",
    "for i, (mean, weight) in enumerate(zip(posterior_means, posterior_weights)):\n",
    "    plt.errorbar(\n",
    "        mean, 0, xerr=jnp.sqrt(PRIOR_VARIANCE), fmt=\"o\", color=f\"C{i}\", capsize=5\n",
    "    )\n",
    "\n",
    "plt.legend(loc=\"upper center\", bbox_to_anchor=(0.5, -0.15), ncol=3)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
