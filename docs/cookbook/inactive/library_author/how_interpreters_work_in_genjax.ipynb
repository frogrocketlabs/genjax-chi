{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import genjax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "genjax.incremental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "genjax.incremental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One wart arises immediately:\n",
    "# * one wart is that -- the signatures of update and regenerate\n",
    "#   means that the concerns of \"doing a Monte Carlo update\"\n",
    "#   tied with incremental computation in Gen.\n",
    "#\n",
    "# Feras really hates about Gen.\n",
    "\n",
    "# class YourGenFn(GenerativeFunction):\n",
    "#     # Non-incremental\n",
    "#     def edit(..., args: Tuple):\n",
    "#         pass\n",
    "\n",
    "#     # Incremental tracediff (monoid) stuff\n",
    "#     #\n",
    "#     # Still doesn't solve \"separation of concerns\"\n",
    "#     # we're computing the \"incremental R-N derivative\" of\n",
    "#     # a generative function ... and maybe that concept\n",
    "#     # we can't disentangle \"incremental changes to values\"\n",
    "#     # from \"incremental changes to traces\"\n",
    "#     #\n",
    "#     # Distinguish between two types of \"diffs\":\n",
    "#     # * Tracediff -- the monoid of tangents on traces\n",
    "#     # * Valuediff -- a monoid of tangents on values\n",
    "#     #   * UnknownChange, NoChange live in.\n",
    "#     # Related to: static incremental lambda calculus\n",
    "\n",
    "#     # score in trace is a w s.t. ..\n",
    "#     # depends how/when it was computed. usually it's either\n",
    "#     # the simulate or the assess guarantee, idk if it's consistent\n",
    "#     # vs `edit` which has a very consistent guarantee propagation\n",
    "\n",
    "#     # score <- simulate(...).get_score()\n",
    "#     # score == log p(chm, x)\n",
    "#     #    where chm ~ p(\\cdot; x)\n",
    "#     #\n",
    "#     # when there is untraced randomness:\n",
    "#     # log_score == log p(chm, r; x) - log q(r; x)\n",
    "#     # guarantee: E_{p(chm | r; x)}[ 1 / score] = 1 / p(chm; x)\n",
    "#     # score = p(chm, r; x) / q(r; x)\n",
    "#     # E_{p(chm | r; x)}[1 / score]\n",
    "#     #   = E_{...}[q(r; x) / p(chm, r; x)]\n",
    "#     #   = \\int q(r; x) (1 / p(chm; x)) dr\n",
    "#     #   = (1 / p(chm; x)) \\int q(r; x) dr = 1 / p(chm; x)\n",
    "#     #\n",
    "#     #\n",
    "#     # tr, _ <- importance(..., chm) -- provided constraint to importance\n",
    "#     # score = tr.get_score()\n",
    "#     # score == log p(chm', x)\n",
    "#     # where chm' ~ q(\\cdot; chm, x)\n",
    "#     #\n",
    "#     # gen_fn had q(\\cdot; chm, x) as the internal proposal\n",
    "#     # SIRCombinator(gen_fn)\n",
    "#     def edif(..., diffs: Diff)\n",
    "#         -> tuple[td: Tracediff, retdiff: Diff, ...]:\n",
    "#         pass\n",
    "\n",
    "#     def update(..., args: Diff)\n",
    "#         -> tuple[..., retval: Diff]:\n",
    "#         pass\n",
    "\n",
    "#     def regenerate(..., args: Diff)\n",
    "#         -> tuple[..., retval: Diff]:\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "genjax.core.interpreters.incremental  # program transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn(x, y):\n",
    "    x = x + y\n",
    "    q = x**2\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "jaxpr = jax.make_jaxpr(fn)(3.0, 3.0)\n",
    "jaxpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# genjax.core.interpreters.incremental(fn)(\n",
    "#     None,\n",
    "#     (SomePytree(3.0), ),\n",
    "#     (SomePytree(genjax.incremental.NoChange), )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  jvp(vmap(fn))\n",
    "# how many times do you make a jaxpr?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it depends on how vmap and jvp are implemented as transformations\n",
    "# vmap(fn) -> Jaxpr -> this Jaxpr will be different than `make_jaxpr(fn)`\n",
    "# jvp(vmap(fn)) -> your making 2 Jaxprs if jvp also requires a Jaxpr as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn(x, y):\n",
    "    z = x + y\n",
    "    q = jax.vmap(lambda z: z**2)(z)\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.make_jaxpr(fn)(jnp.ones(5), 1.0)  # will get shape ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.make_jaxpr(fn)(jnp.ones(10), 1.0)  # will get shape ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.make_jaxpr(fn)(jnp.ones((5, 5)), 1.0)  # lambda z: z ** 2 will get shape (5,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x):\n",
    "    v = x**2\n",
    "    return v\n",
    "\n",
    "\n",
    "def fn(x):\n",
    "    return g(x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "genjax.trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "@genjax.gen\n",
    "def g(x):\n",
    "    x = genjax.trace((\"x\",), genjax.normal, (x, 1.0))  # GenJAX primitive\n",
    "    return x**2\n",
    "\n",
    "\n",
    "@genjax.gen\n",
    "def f(x):\n",
    "    v = genjax.trace((\"v\",), g.vmap(in_axes=(0,)), (x,))  # GenJAX primitive\n",
    "    q = v * 3\n",
    "    y = q**2\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.make_jaxpr(f.source)(\n",
    "    jnp.ones(5),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.make_jaxpr(f.simulate)(jax.random.key(1), (jnp.ones(5),))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
