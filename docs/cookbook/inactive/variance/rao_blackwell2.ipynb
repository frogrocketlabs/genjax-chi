{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rao-Blackwellisation\n",
    "## From high variance low compute to high compute low variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will revisit the Gen1D model and use a Collapsed Gibbs sampler for inference.\n",
    "\n",
    "\n",
    "The idea is that we marginalize out the cluster weights \"probs\" and means \"means\".\n",
    "Following the Rao-Blackwell theorem, this should reduce the variance of our estimate, meaning this should lead to faster mixing and better exploration of the tails of the posterior distribution. \n",
    "We may then directly determine the predictive distribution of cluster assignment [\"idx\", i] given\n",
    "the other cluster assignments \"idx\", and construct a more efficient sampler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See Graphical Models for Visual Object Recognition and Tracking by Erik B. Sudderth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "\n",
    "import genjax\n",
    "from genjax import ChoiceMapBuilder as C\n",
    "from genjax import categorical, dirichlet, gen, normal, pretty\n",
    "from genjax._src.core.pytree import Const, Pytree\n",
    "\n",
    "pretty()\n",
    "key = jax.random.key(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: There are a few ways to revise our model to do this. \n",
    "First, we can decide to not change the model and adapt inference to ignore the sampled \"idx\" values and use the collapsed-Gibbs logic on the rest.\n",
    "We can then overwrite the initially sampled values for \"idx\" with a sample from the approximate posterior.\n",
    "\n",
    "TODO: we could try  partially collapsed Gibbs sampler. It should still be parallel friendly otherwise there's no real point in GenJAX, at least not until much later in the development.\n",
    "- if we try to marginalize out the cluster weights, we then only have 2 Gibbs update:\n",
    "  - update cluster means: no change\n",
    "  - update cluster assignment: \n",
    "  $\\begin{equation}\n",
    "    P([\"idx\", i] ~|~ \"idx\"_{-i}, \"obs\",{\\footnotesize\\text{ALPHA}}, \"means\") \\propto P([\"idx\", i] ~|~ \"idx\", {\\footnotesize\\text{ALPHA}})  P([\"obs\", i] ~|~ \"idx\", \"obs\"_{-i}, {\\footnotesize\\text{(PRIOR\\_MEAN,PRIOR\\_VARIANCE)}})\n",
    "    \\end{equation}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Markov structure, we have the following factorization\n",
    "$\\begin{equation}\n",
    "P([\"idx\", i] ~|~ \"idx\"_{-i}, \"obs\",{\\footnotesize\\text{ALPHA}}, {\\footnotesize\\text{(PRIOR\\_MEAN,PRIOR\\_VARIANCE)}}) \\propto P([\"idx\", i] ~|~ \"idx\", {\\footnotesize\\text{ALPHA}})  P([\"obs\", i] ~|~ \"idx\", \"obs\"_{-i}, {\\footnotesize\\text{(PRIOR\\_MEAN,PRIOR\\_VARIANCE)}})\n",
    "\\end{equation}$\n",
    "\n",
    "For the first term, we marginalize the mixture weights and it is given by:\n",
    "$\\begin{equation}\n",
    "P([\"idx\", i] = k ~|~ \"idx\"_{-i}, {\\footnotesize\\text{ALPHA}}) = \\frac{N_k^{-i}+{\\footnotesize\\text{ALPHA}} /{\\footnotesize\\text{N\\_CLUSTERS}}}{{\\footnotesize\\text{N\\_DATAPOINTS}}-1+ {\\footnotesize\\text{ALPHA}}}\n",
    "\\end{equation}$\n",
    "where $N_k^{-i}$ is the current number of obs assigned to the cluster $k$, excluding $[\"idx\", i]$.\n",
    "\n",
    "For the second term, we have:\n",
    "$\\begin{equation}\n",
    "P([\"obs\", i] ~|~ [\"idx\", i] = k, \"idx\"_{-i}, \"obs\"_{-i}, {\\footnotesize\\text{(PRIOR\\_MEAN,PRIOR\\_VARIANCE)}}) = P([\"obs\", i] ~|~ \\{[\"obs\", j] ~|~ [\"idx\",j]=k,j\\neq i\\} ,{\\footnotesize\\text{(PRIOR\\_MEAN,PRIOR\\_VARIANCE)}})\n",
    "\\end{equation}$\n",
    "\n",
    "where the last one for us will be a a Student–t predictive distribution, which can usually be approximated by moment–matched Gaussians.\n",
    "\n",
    "The resulting new Gibbs sweep looks like:\n",
    "1. Sample a random permutation $\\tau$ of $\\{1,...,{\\footnotesize\\text{N\\_DATAPOINTS}}\\}$\n",
    "2. For each i in  $\\{\\tau(1),...,\\tau({\\footnotesize\\text{N\\_DATAPOINTS}})\\}$\n",
    "   1.  For each of the clusters, compute the predictive likelihood (2) (possibly from cached sufficient statistics)\n",
    "   2.  Sample a new cluster assignment for each point from a categorical with values from (1)\n",
    "   3.  Update cached sufficient statistics\n",
    "\n",
    "At the end, optionally, we can sample mixture parameters and cluster means via the previous Gibbs sampling scheme and the current assignment of the datapoints.\n",
    "\n",
    "One could also do a partially collapsed Gibbs sampler for more parallelization, or do the wrong bloc-Gibbs update with an MH accept ratio for more parallelism in the number of datapoints (again a coarse to fine inference using a style of Hogwild Gibbs sampling). \n",
    "Could look at:\n",
    "- https://www.phontron.com/paper/neubig14pgibbs.pdf\n",
    "- https://www.sciencedirect.com/science/article/pii/S0167865517300752?casa_token=nYtOwi2vkpEAAAAA:EI_UgTLBHlM4bqwbH4YkXEeHAuGCIEkRcA6WOgGQxYxtyDbNNAl23AKEQtvVNwz0_1oZxxu5\n",
    "- https://cs.brown.edu/~sudderth/papers/sudderthPhD.pdf p90-94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "PRIOR_VARIANCE = 10.0\n",
    "OBS_VARIANCE = 1.0\n",
    "N_DATAPOINTS = 5000\n",
    "N_CLUSTERS = 40\n",
    "ALPHA = float(N_DATAPOINTS / (N_CLUSTERS * 10))\n",
    "PRIOR_MEAN = 50.0\n",
    "N_ITER = 50\n",
    "\n",
    "# Debugging mode\n",
    "DEBUG = True\n",
    "\n",
    "\n",
    "@Pytree.dataclass\n",
    "class Cluster(Pytree):\n",
    "    mean: float\n",
    "\n",
    "\n",
    "@gen\n",
    "def generate_cluster(mean, var):\n",
    "    cluster_mean = normal(mean, var) @ \"mean\"\n",
    "    return Cluster(cluster_mean)\n",
    "\n",
    "\n",
    "@gen\n",
    "def generate_cluster_weight(alphas):\n",
    "    probs = dirichlet(alphas) @ \"probs\"\n",
    "    return probs\n",
    "\n",
    "\n",
    "@gen\n",
    "def generate_datapoint(probs, clusters):\n",
    "    idx = categorical(jnp.log(probs)) @ \"idx\"\n",
    "    obs = normal(clusters.mean[idx], OBS_VARIANCE) @ \"obs\"\n",
    "    return obs\n",
    "\n",
    "\n",
    "@gen\n",
    "def generate_data(n_clusters: Const[int], n_datapoints: Const[int], alpha: float):\n",
    "    clusters = (\n",
    "        generate_cluster.repeat(n=n_clusters.unwrap())(PRIOR_MEAN, PRIOR_VARIANCE)\n",
    "        @ \"clusters\"\n",
    "    )\n",
    "\n",
    "    probs = generate_cluster_weight.inline(\n",
    "        alpha / n_clusters.unwrap() * jnp.ones(n_clusters.unwrap())\n",
    "    )\n",
    "\n",
    "    datapoints = (\n",
    "        generate_datapoint.repeat(n=n_datapoints.unwrap())(probs, clusters)\n",
    "        @ \"datapoints\"\n",
    "    )\n",
    "\n",
    "    return datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoints = C[\"datapoints\", \"obs\"].set(\n",
    "    jnp.concatenate([\n",
    "        jax.random.uniform(jax.random.key(i), shape=(int(N_DATAPOINTS / N_CLUSTERS),))\n",
    "        + PRIOR_MEAN\n",
    "        + PRIOR_VARIANCE * (-4 + 8 * i / N_CLUSTERS)\n",
    "        for i in range(N_CLUSTERS)\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(datapoints):\n",
    "    key = jax.random.key(32421)\n",
    "    args = (Const(N_CLUSTERS), Const(N_DATAPOINTS), ALPHA)\n",
    "    key, subkey = jax.random.split(key)\n",
    "    initial_weights = C[\"probs\"].set(jnp.ones(N_CLUSTERS) / N_CLUSTERS)\n",
    "    constraints = datapoints | initial_weights\n",
    "    tr, _ = generate_data.importance(subkey, constraints, args)\n",
    "\n",
    "    if DEBUG:\n",
    "        all_posterior_means = [tr.get_choices()[\"clusters\", \"mean\"]]\n",
    "        all_posterior_weights = [tr.get_choices()[\"probs\"]]\n",
    "        all_cluster_assignment = [tr.get_choices()[\"datapoints\", \"idx\"]]\n",
    "\n",
    "        for _ in range(N_ITER):\n",
    "            # Gibbs update on `(\"clusters\", i, \"mean\")` for each i, in parallel\n",
    "            key, subkey = jax.random.split(key)\n",
    "            tr = jax.jit(update_cluster_means)(subkey, tr)\n",
    "            all_posterior_means.append(tr.get_choices()[\"clusters\", \"mean\"])\n",
    "\n",
    "            # # Gibbs update on `(\"datapoints\", i, \"idx\")` for each `i`, in parallel\n",
    "            key, subkey = jax.random.split(key)\n",
    "            tr = jax.jit(update_datapoint_assignment)(subkey, tr)\n",
    "            all_cluster_assignment.append(tr.get_choices()[\"datapoints\", \"idx\"])\n",
    "\n",
    "            # # Gibbs update on `probs`\n",
    "            key, subkey = jax.random.split(key)\n",
    "            tr = jax.jit(update_cluster_weights)(subkey, tr)\n",
    "            all_posterior_weights.append(tr.get_choices()[\"probs\"])\n",
    "\n",
    "        return all_posterior_means, all_posterior_weights, all_cluster_assignment, tr\n",
    "\n",
    "    else:\n",
    "\n",
    "        def update(carry, _):\n",
    "            key, tr = carry\n",
    "            key, subkey = jax.random.split(key)\n",
    "            tr = jax.jit(update_cluster_means)(subkey, tr)\n",
    "\n",
    "            key, subkey = jax.random.split(key)\n",
    "            tr = jax.jit(update_datapoint_assignment)(subkey, tr)\n",
    "\n",
    "            key, subkey = jax.random.split(key)\n",
    "            tr = jax.jit(update_cluster_weights)(subkey, tr)\n",
    "            return (key, tr), None\n",
    "\n",
    "        (key, tr), _ = jax.lax.scan(update, (key, tr), None, length=N_ITER)\n",
    "        return tr\n",
    "\n",
    "\n",
    "def update_cluster_means(key, trace):\n",
    "    datapoint_indexes = trace.get_choices()[\"datapoints\", \"idx\"]\n",
    "    datapoints = trace.get_choices()[\"datapoints\", \"obs\"]\n",
    "    n_clusters = trace.get_args()[0].unwrap()\n",
    "    current_means = trace.get_choices()[\"clusters\", \"mean\"]\n",
    "\n",
    "    category_counts = jnp.bincount(\n",
    "        trace.get_choices()[\"datapoints\", \"idx\"],\n",
    "        length=n_clusters,\n",
    "        minlength=n_clusters,\n",
    "    )\n",
    "\n",
    "    cluster_means = (\n",
    "        jax.vmap(\n",
    "            lambda i: jnp.sum(jnp.where(datapoint_indexes == i, datapoints, 0)),\n",
    "            in_axes=(0),\n",
    "            out_axes=(0),\n",
    "        )(jnp.arange(n_clusters))\n",
    "        / category_counts\n",
    "    )\n",
    "\n",
    "    posterior_means = (\n",
    "        PRIOR_VARIANCE\n",
    "        / (PRIOR_VARIANCE + OBS_VARIANCE / category_counts)\n",
    "        * cluster_means\n",
    "        + (OBS_VARIANCE / category_counts)\n",
    "        / (PRIOR_VARIANCE + OBS_VARIANCE / category_counts)\n",
    "        * PRIOR_MEAN\n",
    "    )\n",
    "\n",
    "    posterior_variances = 1 / (1 / PRIOR_VARIANCE + category_counts / OBS_VARIANCE)\n",
    "\n",
    "    key, subkey = jax.random.split(key)\n",
    "    new_means = (\n",
    "        generate_cluster.vmap()\n",
    "        .simulate(key, (posterior_means, posterior_variances))\n",
    "        .get_choices()[\"mean\"]\n",
    "    )\n",
    "\n",
    "    chosen_means = jnp.where(category_counts == 0, current_means, new_means)\n",
    "\n",
    "    argdiffs = genjax.Diff.no_change(trace.args)\n",
    "    new_trace, _, _, _ = trace.update(\n",
    "        subkey, C[\"clusters\", \"mean\"].set(chosen_means), argdiffs\n",
    "    )\n",
    "    return new_trace\n",
    "\n",
    "\n",
    "def update_datapoint_assignment(key, trace):\n",
    "    def compute_local_density(x, i):\n",
    "        datapoint_mean = trace.get_choices()[\"datapoints\", \"obs\", x]\n",
    "        chm = C[\"obs\"].set(datapoint_mean).at[\"idx\"].set(i)\n",
    "        clusters = Cluster(trace.get_choices()[\"clusters\", \"mean\"])\n",
    "        probs = trace.get_choices()[\"probs\"]\n",
    "        args = (probs, clusters)\n",
    "        model_logpdf, _ = generate_datapoint.assess(chm, args)\n",
    "        return model_logpdf\n",
    "\n",
    "    n_clusters = trace.get_args()[0].unwrap()\n",
    "    n_datapoints = trace.get_args()[1].unwrap()\n",
    "    local_densities = jax.vmap(\n",
    "        lambda x: jax.vmap(lambda i: compute_local_density(x, i))(\n",
    "            jnp.arange(n_clusters)\n",
    "        )\n",
    "    )(jnp.arange(n_datapoints))\n",
    "\n",
    "    key, subkey = jax.random.split(key)\n",
    "    new_datapoint_indexes = (\n",
    "        genjax.categorical.vmap().simulate(key, (local_densities,)).get_choices()\n",
    "    )\n",
    "    argdiffs = genjax.Diff.no_change(trace.args)\n",
    "    new_trace, _, _, _ = trace.update(\n",
    "        subkey, C[\"datapoints\", \"idx\"].set(new_datapoint_indexes), argdiffs\n",
    "    )\n",
    "    return new_trace\n",
    "\n",
    "\n",
    "def update_cluster_weights(key, trace):\n",
    "    n_clusters = trace.get_args()[0].unwrap()\n",
    "    category_counts = jnp.bincount(\n",
    "        trace.get_choices()[\"datapoints\", \"idx\"],\n",
    "        length=n_clusters,\n",
    "        minlength=n_clusters,\n",
    "    )\n",
    "\n",
    "    new_alpha = ALPHA / n_clusters * jnp.ones(n_clusters) + category_counts\n",
    "    key, subkey = jax.random.split(key)\n",
    "    new_probs = generate_cluster_weight.simulate(key, (new_alpha,)).get_retval()\n",
    "\n",
    "    argdiffs = genjax.Diff.no_change(trace.args)\n",
    "    new_trace, _, _, _ = trace.update(subkey, C[\"probs\"].set(new_probs), argdiffs)\n",
    "    return new_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    (\n",
    "        all_posterior_means,\n",
    "        all_posterior_weights,\n",
    "        all_cluster_assignment,\n",
    "        posterior_trace,\n",
    "    ) = infer(datapoints)\n",
    "else:\n",
    "    posterior_trace = infer(datapoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create plot for a given index\n",
    "def create_plot(idx):\n",
    "    datapoint = datapoints[\"datapoints\", \"obs\"]\n",
    "    posterior_means = all_posterior_means[idx]\n",
    "    posterior_weights = all_posterior_weights[idx]\n",
    "    cluster_assignment = all_cluster_assignment[idx]\n",
    "\n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "    # Plot datapoints colored by cluster assignment and posterior means together\n",
    "    for i in range(len(posterior_means)):\n",
    "        # Only plot points assigned to this cluster\n",
    "\n",
    "        mask = cluster_assignment == i\n",
    "        n_points = jnp.sum(mask)  # Count points in this cluster\n",
    "\n",
    "        if not jnp.any(mask):  # Skip if no points assigned to this cluster\n",
    "            continue\n",
    "\n",
    "        # Add random jitter to y-coordinates\n",
    "        key = jax.random.PRNGKey(i)  # Use cluster index as seed\n",
    "        y_jitter = jax.random.uniform(\n",
    "            key, shape=datapoint[mask].shape, minval=-0.1, maxval=0.1\n",
    "        )\n",
    "\n",
    "        ax.scatter(\n",
    "            datapoint[mask],\n",
    "            y_jitter,  # Use jittered y-coordinates\n",
    "            color=f\"C{i}\",\n",
    "            alpha=0.3,\n",
    "            s=5,\n",
    "        )\n",
    "\n",
    "        # Plot posterior means with size proportional to weights\n",
    "        weight = posterior_weights[i]  # Get current weight for this iteration\n",
    "        ax.scatter(\n",
    "            posterior_means[i],\n",
    "            0,\n",
    "            color=f\"C{i}\",\n",
    "            marker=\"*\",\n",
    "            s=300 + weight * 1200,  # Made cluster means much bigger\n",
    "            alpha=1,\n",
    "            label=f\"Cluster {i + 1} (Prob: {weight:.6f}, Points: {n_points})\",  # Added point count\n",
    "        )\n",
    "\n",
    "        # Plot standard deviation of the Gaussian means\n",
    "        ax.errorbar(\n",
    "            posterior_means[i],\n",
    "            0,\n",
    "            xerr=jnp.sqrt(OBS_VARIANCE),\n",
    "            fmt=\"o\",\n",
    "            color=f\"C{i}\",\n",
    "            capsize=5,\n",
    "        )\n",
    "\n",
    "    ax.legend(loc=\"upper center\", bbox_to_anchor=(0.5, -0.15), ncol=3)\n",
    "    ax.set_title(f\"Iteration {idx}\")\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "NUM_FRAMES = 50\n",
    "\n",
    "# Create animation\n",
    "frames = []\n",
    "for i in range(NUM_FRAMES):\n",
    "    fig = create_plot(int(N_ITER / NUM_FRAMES * i))\n",
    "    # Convert figure to image array\n",
    "    fig.canvas.draw()\n",
    "    image = np.frombuffer(\n",
    "        fig.canvas.buffer_rgba(), dtype=np.uint8\n",
    "    )  # Updated to use buffer_rgba\n",
    "    image = image.reshape(\n",
    "        fig.canvas.get_width_height()[::-1] + (4,)\n",
    "    )  # Note: buffer_rgba returns RGBA\n",
    "    frames.append(image[:, :, :3])  # Convert RGBA to RGB by dropping alpha channel\n",
    "    plt.close(fig)\n",
    "\n",
    "# Create animation from frames\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "from matplotlib import animation\n",
    "\n",
    "ani = animation.ArtistAnimation(\n",
    "    fig,\n",
    "    [[plt.imshow(frame)] for frame in frames],\n",
    "    interval=200,  # .2 second between frames\n",
    "    blit=True,\n",
    ")\n",
    "\n",
    "# Save animation as GIF\n",
    "imageio.mimsave(\"dirichlet_mixture_animation.gif\", frames, fps=15)\n",
    "\n",
    "# Display animation in notebook\n",
    "HTML(ani.to_jshtml())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
