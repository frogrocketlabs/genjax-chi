{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Hamiltonian Monte Carlo (MCMC)\n",
    "subtitle: How to write advanced custom inference in GenJAX\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit\n",
    "from jax import tree_util as jtu\n",
    "\n",
    "import genjax\n",
    "from genjax import ChoiceMap, GenerativeFunction, pretty\n",
    "from genjax import ChoiceMapBuilder as C\n",
    "from genjax import SelectionBuilder as S\n",
    "from genjax.typing import static_check_supports_grad\n",
    "\n",
    "pretty()\n",
    "key = jax.random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a class `SymplecticIntegrator`. Integrators solve differential equations by taking a point and integrating along the path defined by the differential equation to the solution at some time `t`. It is symplectic when it satisfies a particular equation that is useful for the stablity of the solution of certain dynamical systems, and in particular the one used in the HMC algorithm.\n",
    "\n",
    "The particular we will implement is the classic leap frop integrator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SymplecticIntegrator:\n",
    "    def integrate(potential, kinetic, init_q, init_p):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "# Vector space structure of choice maps\n",
    "# choice maps are the data structures that store all the random variables\n",
    "# described by a Gen model. Each trace contains a choice map and some other\n",
    "# metadata\n",
    "def chm_scale(c: float, chm: ChoiceMap) -> ChoiceMap:\n",
    "    return jtu.tree_map(lambda x: c * x, chm)\n",
    "\n",
    "\n",
    "def chm_add(chm1: ChoiceMap, chm2: ChoiceMap) -> ChoiceMap:\n",
    "    return jtu.tree_map(lambda x, y: x + y, chm1, chm2)\n",
    "\n",
    "\n",
    "# Utility functions\n",
    "def tree_zip(diff_tree, nondiff_tree):\n",
    "    return jtu.tree_map(\n",
    "        lambda v1, v2: v1 if v1 is not None else v2, diff_tree, nondiff_tree\n",
    "    )\n",
    "\n",
    "\n",
    "def my_closure(f, diff_tree, nondiff_tree):\n",
    "    full_tree = tree_zip(diff_tree, nondiff_tree)\n",
    "    return f(full_tree)\n",
    "\n",
    "\n",
    "def get_diff_tree(tree):\n",
    "    return jtu.tree_map(lambda v: v if static_check_supports_grad(v) else None, tree)\n",
    "\n",
    "\n",
    "def get_nondiff_tree(tree):\n",
    "    return jtu.tree_map(\n",
    "        lambda v: v if not static_check_supports_grad(v) else None, tree\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ChmLeapFrog(SymplecticIntegrator):\n",
    "    \"\"\"\n",
    "    A leap frog integrator on the space of choice maps\n",
    "    \"\"\"\n",
    "\n",
    "    step_size: float\n",
    "    num_steps: int\n",
    "\n",
    "    def integrate(self, potential, kinetic, init_q, init_p):\n",
    "        non_diff_q = get_nondiff_tree(init_q)\n",
    "        non_diff_p = get_nondiff_tree(init_p)\n",
    "\n",
    "        def diff_potential(x):\n",
    "            return my_closure(potential, x, non_diff_q)\n",
    "\n",
    "        def diff_kinetic(x):\n",
    "            return my_closure(kinetic, x, non_diff_p)\n",
    "\n",
    "        dUdq = jax.grad(diff_potential)\n",
    "        dKdp = jax.grad(diff_kinetic)\n",
    "\n",
    "        def step(qp, _):\n",
    "            q, p = qp\n",
    "            p_halfway = chm_add(\n",
    "                p, get_diff_tree(chm_scale(-self.step_size / 2, dUdq(q)))\n",
    "            )\n",
    "            next_q = chm_add(\n",
    "                q, get_diff_tree(chm_scale(self.step_size, dKdp(p_halfway)))\n",
    "            )\n",
    "            next_p = chm_add(\n",
    "                p_halfway, get_diff_tree(chm_scale(self.step_size / 2, dUdq(next_q)))\n",
    "            )\n",
    "            return (next_q, next_p), None\n",
    "\n",
    "        return jax.lax.scan(step, (init_q, init_p), length=self.num_steps)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a class `HMCSampler` that will run HMC on the choicemap of a generative function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class HMCSampler:\n",
    "    model: GenerativeFunction  # the joint model\n",
    "    model_args: Tuple  # the arguments of the model (e.g. hyperparameters)\n",
    "    data: ChoiceMap  # the data on which we are conditioning\n",
    "    integrator: SymplecticIntegrator  # the symplectic integration strategy\n",
    "    mass: float = 1.0\n",
    "\n",
    "    def potential(self, params: ChoiceMap) -> float:\n",
    "        result = self.model.importance(\n",
    "            jax.random.PRNGKey(0), params.merge(self.data), self.model_args\n",
    "        )[0].get_score()\n",
    "        return -result\n",
    "\n",
    "    def kinetic(self, momentum: ChoiceMap) -> float:\n",
    "        return jtu.tree_reduce(lambda s, x: s + x**2, momentum, 0) / self.mass / 2\n",
    "\n",
    "    def sample_momentum(self, key) -> ChoiceMap:\n",
    "        keys = jax.random.split(key, self.param_structure.num_leaves)\n",
    "        momentum = jax.vmap(jax.random.normal)(keys) * self.mass\n",
    "        return jtu.tree_unflatten(self.param_structure, momentum)\n",
    "\n",
    "    def hamiltonian(self, params: ChoiceMap, momentum: ChoiceMap) -> float:\n",
    "        potential_value = self.potential(params)\n",
    "        kinetic_value = self.kinetic(momentum)\n",
    "        return potential_value + kinetic_value\n",
    "\n",
    "    def hmc_step(self, key, params) -> Tuple[ChoiceMap, bool]:\n",
    "        momentum_key, mh_key = jax.random.split(key)\n",
    "        momentum = self.sample_momentum(momentum_key)\n",
    "        hamiltonian = self.hamiltonian(params, momentum)\n",
    "        new_params, new_momentum = self.integrator.integrate(\n",
    "            self.potential, self.kinetic, params, momentum\n",
    "        )\n",
    "        new_hamiltonian = self.hamiltonian(new_params, new_momentum)\n",
    "        log_mh_ratio = hamiltonian - new_hamiltonian\n",
    "        acc = jnp.log(jax.random.uniform(mh_key)) < log_mh_ratio\n",
    "        return jax.lax.cond(acc, lambda: new_params, lambda: params), acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write a simple generative function on which to test our HMC algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@genjax.gen\n",
    "def model():\n",
    "    x = genjax.normal(0.0, 4.0) @ \"x\"\n",
    "    y = genjax.normal(x, 1.0) @ \"y\"\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally test HMC on our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_params = C[\"x\"].set(0.0)\n",
    "data = C[\"y\"].set(1.0)\n",
    "leap_size = 1e-3\n",
    "num_steps = 40\n",
    "hmc_sampler = HMCSampler(model, (), data, ChmLeapFrog(leap_size, num_steps))\n",
    "\n",
    "hmc_sampler.hmc_step(key, init_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also test for a slightly more complex model, e.g. an HMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_chain = 50\n",
    "state_size = 50\n",
    "number_runs = 100\n",
    "# for numerical stability of the HMM, ensuring that the eigenvalues of the transition matrices are around 1.\n",
    "magic_number = jnp.exp(1)\n",
    "normalizer = 1.0 / jnp.sqrt(state_size / magic_number)\n",
    "key, subkey = jax.random.split(key)\n",
    "transition_matrix = jax.random.normal(subkey, (state_size, state_size)) * normalizer\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "observation_matrix = jax.random.normal(subkey, (state_size, state_size)) * normalizer\n",
    "latent_variance = jnp.eye(state_size)\n",
    "obs_variance = jnp.eye(state_size)\n",
    "key, subkey = jax.random.split(key)\n",
    "\n",
    "\n",
    "@genjax.gen\n",
    "def initial_state_model():\n",
    "    return (\n",
    "        genjax.mv_normal(\n",
    "            jnp.zeros(state_size, dtype=float), jnp.identity(state_size, dtype=float)\n",
    "        )\n",
    "        @ \"initial_state\"\n",
    "    )\n",
    "\n",
    "\n",
    "@genjax.gen\n",
    "def hmm_step(x, _):\n",
    "    new_x = (\n",
    "        genjax.mv_normal(jnp.matmul(transition_matrix, x), latent_variance) @ \"new_x\"\n",
    "    )\n",
    "    _ = genjax.mv_normal(jnp.matmul(observation_matrix, new_x), obs_variance) @ \"obs\"\n",
    "    return new_x, None\n",
    "\n",
    "\n",
    "@genjax.gen\n",
    "def hmm():\n",
    "    x = initial_state_model() @ \"init\"\n",
    "    _ = hmm_step.scan(n=length_chain)(x, None) @ \"steps\"\n",
    "\n",
    "\n",
    "# Testing that the model runs\n",
    "jitted = jit(hmm.repeat(n=number_runs).simulate)\n",
    "key, subkey = jax.random.split(key)\n",
    "trace = jitted(subkey, ())\n",
    "trace.get_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating observations\n",
    "obs = jax.vmap(\n",
    "    lambda idx: C[\"steps\", idx, \"obs\"].set(\n",
    "        idx.astype(float) * jnp.arange(state_size) / state_size\n",
    "    )\n",
    ")(jnp.arange(length_chain))\n",
    "\n",
    "# Creating an initial state\n",
    "key, subkey = jax.random.split(key)\n",
    "init_params = (\n",
    "    hmm.importance(subkey, obs, ())[0]\n",
    "    .get_choices()\n",
    "    .filter(S[\"initial_state\"] | S[\"steps\", ..., \"new_x\"])\n",
    "    .simplify()\n",
    ")\n",
    "\n",
    "# type of init_params\n",
    "param_structure = jtu.tree_structure(init_params)\n",
    "\n",
    "\n",
    "# Running HMC\n",
    "hmc_sampler = HMCSampler(\n",
    "    hmm, (), obs, param_structure, ChmLeapFrog(leap_size, num_steps)\n",
    ")\n",
    "key, subkey = jax.random.split(key)\n",
    "jit(hmc_sampler.hmc_step)(subkey, init_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, note that this is not a tutorial on HMC. Typical implementations have many more parameters, including a burn-in period, a way to tune the step size and number of steps, and more sophisticated methods to decide when to stop the algorithm. This simpler version is only slightly fancier version of the typical Metropolis-Hastings random-walk algorithm that one could use as a rejuvenation kernel in an SMC algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
